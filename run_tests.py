#!/usr/bin/env python3
"""
å•†ç”¨ç´šæ¸¬è©¦åŸ·è¡Œå™¨
ä¸€éµåŸ·è¡Œæ‰€æœ‰æ¸¬è©¦ä¸¦ç”Ÿæˆå®Œæ•´å ±å‘Š
"""

import os
import sys
import json
import time
import argparse
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import webbrowser

# é¡è‰²è¼¸å‡º
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'

def print_colored(text: str, color: str = Colors.WHITE):
    """åˆ—å°æœ‰é¡è‰²çš„æ–‡å­—"""
    print(f"{color}{text}{Colors.END}")

def print_section(title: str):
    """åˆ—å°å€æ®µæ¨™é¡Œ"""
    print_colored(f"\n{'='*60}", Colors.CYAN)
    print_colored(f"  {title}", Colors.CYAN + Colors.BOLD)
    print_colored(f"{'='*60}", Colors.CYAN)

def run_command(cmd: List[str], description: str = None) -> Dict[str, Any]:
    """åŸ·è¡Œå‘½ä»¤ä¸¦å›å‚³çµæœ"""
    if description:
        print_colored(f"â–¶ {description}", Colors.BLUE)
    
    start_time = time.time()
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False,
            encoding='utf-8'
        )
        
        duration = time.time() - start_time
        
        return {
            'success': result.returncode == 0,
            'returncode': result.returncode,
            'stdout': result.stdout,
            'stderr': result.stderr,
            'duration': duration,
            'command': ' '.join(cmd)
        }
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'duration': time.time() - start_time,
            'command': ' '.join(cmd)
        }

class TestSuite:
    """æ¸¬è©¦å¥—ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        
    def run_all_tests(self, args):
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print_colored("ğŸ§ª SRT GO å•†ç”¨ç´šæ¸¬è©¦å¥—ä»¶", Colors.BOLD + Colors.MAGENTA)
        print_colored(f"å°ˆæ¡ˆè·¯å¾‘: {self.project_root}", Colors.WHITE)
        print_colored(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", Colors.WHITE)
        
        self.start_time = time.time()
        
        # æª¢æŸ¥ç’°å¢ƒ
        if not self.check_environment():
            return False
        
        # åŸ·è¡Œæ¸¬è©¦éšæ®µ
        test_phases = []
        
        if not args.skip_unit:
            test_phases.append(('unit_tests', 'å–®å…ƒæ¸¬è©¦', self.run_unit_tests))
        
        if not args.skip_integration:
            test_phases.append(('integration_tests', 'æ•´åˆæ¸¬è©¦', self.run_integration_tests))
        
        if not args.skip_e2e:
            test_phases.append(('e2e_tests', 'E2Eæ¸¬è©¦', self.run_e2e_tests))
        
        if not args.skip_performance:
            test_phases.append(('performance_tests', 'æ•ˆèƒ½æ¸¬è©¦', self.run_performance_tests))
        
        if not args.skip_security:
            test_phases.append(('security_tests', 'å®‰å…¨æ¸¬è©¦', self.run_security_tests))
        
        # åŸ·è¡Œæ¸¬è©¦
        all_passed = True
        for test_id, test_name, test_func in test_phases:
            print_section(test_name)
            result = test_func()
            self.test_results[test_id] = result
            
            if result['success']:
                print_colored(f"âœ… {test_name} é€šé ({result['duration']:.1f}s)", Colors.GREEN)
            else:
                print_colored(f"âŒ {test_name} å¤±æ•— ({result['duration']:.1f}s)", Colors.RED)
                all_passed = False
                
                if not args.continue_on_failure:
                    break
        
        self.end_time = time.time()
        
        # ç”Ÿæˆå ±å‘Š
        self.generate_report(args.report_format)
        
        # é¡¯ç¤ºæ‘˜è¦
        self.show_summary()
        
        return all_passed
    
    def check_environment(self) -> bool:
        """æª¢æŸ¥æ¸¬è©¦ç’°å¢ƒ"""
        print_section("ç’°å¢ƒæª¢æŸ¥")
        
        # æª¢æŸ¥ Python
        python_result = run_command([sys.executable, '--version'], "æª¢æŸ¥ Python ç‰ˆæœ¬")
        if not python_result['success']:
            print_colored("âŒ Python ä¸å¯ç”¨", Colors.RED)
            return False
        
        print_colored(f"âœ… {python_result['stdout'].strip()}", Colors.GREEN)
        
        # æª¢æŸ¥å¿…éœ€å¥—ä»¶
        required_packages = [
            'pytest', 'pytest-cov', 'numpy', 'faster-whisper'
        ]
        
        for package in required_packages:
            result = run_command([sys.executable, '-c', f'import {package}'], f"æª¢æŸ¥ {package}")
            if result['success']:
                print_colored(f"âœ… {package} å¯ç”¨", Colors.GREEN)
            else:
                print_colored(f"âŒ {package} ä¸å¯ç”¨", Colors.RED)
                print_colored(f"è«‹åŸ·è¡Œ: pip install {package}", Colors.YELLOW)
                return False
        
        # æª¢æŸ¥æ¸¬è©¦æª”æ¡ˆ
        test_dirs = ['tests/unit', 'tests/integration', 'tests/e2e', 'tests/performance', 'tests/security']
        for test_dir in test_dirs:
            test_path = self.project_root / test_dir
            if test_path.exists():
                count = len(list(test_path.glob('test_*.py')))
                print_colored(f"âœ… {test_dir}: {count} å€‹æ¸¬è©¦æª”æ¡ˆ", Colors.GREEN)
            else:
                print_colored(f"âš ï¸  {test_dir}: ç›®éŒ„ä¸å­˜åœ¨", Colors.YELLOW)
        
        return True
    
    def run_unit_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å…ƒæ¸¬è©¦"""
        cmd = [
            sys.executable, '-m', 'pytest',
            'tests/unit/',
            '--cov=srt_whisper_lite',
            '--cov-report=html:coverage/html',
            '--cov-report=xml:coverage/coverage.xml',
            '--cov-report=term-missing',
            '--junit-xml=reports/unit_tests.xml',
            '-v'
        ]
        
        return run_command(cmd, "åŸ·è¡Œå–®å…ƒæ¸¬è©¦")
    
    def run_integration_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ•´åˆæ¸¬è©¦"""
        cmd = [
            sys.executable, '-m', 'pytest',
            'tests/integration/',
            '--junit-xml=reports/integration_tests.xml',
            '-v'
        ]
        
        return run_command(cmd, "åŸ·è¡Œæ•´åˆæ¸¬è©¦")
    
    def run_e2e_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œç«¯åˆ°ç«¯æ¸¬è©¦"""
        # å…ˆå»ºç½®æ‡‰ç”¨
        build_cmd = ['npm', 'run', 'build']
        build_result = run_command(build_cmd, "å»ºç½®æ‡‰ç”¨ç¨‹å¼")
        
        if not build_result['success']:
            return build_result
        
        cmd = [
            sys.executable, '-m', 'pytest',
            'tests/e2e/',
            '--junit-xml=reports/e2e_tests.xml',
            '-v'
        ]
        
        return run_command(cmd, "åŸ·è¡Œ E2E æ¸¬è©¦")
    
    def run_performance_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦"""
        cmd = [
            sys.executable, '-m', 'pytest',
            'tests/performance/',
            '--benchmark-only',
            '--benchmark-json=reports/benchmark.json',
            '--benchmark-autosave',
            '--junit-xml=reports/performance_tests.xml',
            '-v'
        ]
        
        return run_command(cmd, "åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦")
    
    def run_security_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®‰å…¨æ¸¬è©¦"""
        # åŸ·è¡Œ Safety æª¢æŸ¥
        safety_cmd = ['safety', 'check', '--json']
        safety_result = run_command(safety_cmd, "åŸ·è¡Œ Safety å®‰å…¨æª¢æŸ¥")
        
        # åŸ·è¡Œ Bandit æª¢æŸ¥
        bandit_cmd = ['bandit', '-r', 'srt_whisper_lite/', '-f', 'json', '-o', 'reports/bandit.json']
        bandit_result = run_command(bandit_cmd, "åŸ·è¡Œ Bandit å®‰å…¨æª¢æŸ¥")
        
        # åŸ·è¡Œå®‰å…¨æ¸¬è©¦
        pytest_cmd = [
            sys.executable, '-m', 'pytest',
            'tests/security/',
            '--junit-xml=reports/security_tests.xml',
            '-v'
        ]
        
        pytest_result = run_command(pytest_cmd, "åŸ·è¡Œå®‰å…¨æ¸¬è©¦")
        
        # çµåˆæ‰€æœ‰çµæœ
        success = all([
            safety_result.get('success', False),
            bandit_result.get('success', True),  # Bandit å¯èƒ½å ±å‘Šå•é¡Œä½†ä¸ç®—å¤±æ•—
            pytest_result.get('success', False)
        ])
        
        return {
            'success': success,
            'duration': sum([
                safety_result.get('duration', 0),
                bandit_result.get('duration', 0),
                pytest_result.get('duration', 0)
            ]),
            'details': {
                'safety': safety_result,
                'bandit': bandit_result,
                'pytest': pytest_result
            }
        }
    
    def generate_report(self, format_type: str = 'html'):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print_section("ç”Ÿæˆæ¸¬è©¦å ±å‘Š")
        
        # ç¢ºä¿å ±å‘Šç›®éŒ„å­˜åœ¨
        reports_dir = self.project_root / 'reports'
        reports_dir.mkdir(exist_ok=True)
        
        # æ”¶é›†æ¸¬è©¦çµæœ
        total_duration = self.end_time - self.start_time if self.end_time and self.start_time else 0
        
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'version': '2.2.1',
            'total_duration': total_duration,
            'test_results': self.test_results,
            'summary': self.calculate_summary()
        }
        
        # ç”Ÿæˆ JSON å ±å‘Š
        json_path = reports_dir / 'test_report.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print_colored(f"âœ… JSON å ±å‘Š: {json_path}", Colors.GREEN)
        
        # ç”Ÿæˆ HTML å ±å‘Š
        if format_type in ['html', 'all']:
            html_path = reports_dir / 'test_report.html'
            self.generate_html_report(report_data, html_path)
            print_colored(f"âœ… HTML å ±å‘Š: {html_path}", Colors.GREEN)
        
        return json_path, html_path if format_type in ['html', 'all'] else None
    
    def generate_html_report(self, data: Dict, output_path: Path):
        """ç”Ÿæˆ HTML å ±å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SRT GO æ¸¬è©¦å ±å‘Š</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 8px 8px 0 0; }}
        .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; padding: 20px; }}
        .metric {{ background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; border-left: 4px solid #007bff; }}
        .metric.success {{ border-left-color: #28a745; }}
        .metric.warning {{ border-left-color: #ffc107; }}
        .metric.danger {{ border-left-color: #dc3545; }}
        .test-results {{ padding: 20px; }}
        .test-item {{ margin-bottom: 15px; padding: 15px; border-radius: 5px; }}
        .test-item.passed {{ background: #d4edda; border-left: 4px solid #28a745; }}
        .test-item.failed {{ background: #f8d7da; border-left: 4px solid #dc3545; }}
        .test-item.skipped {{ background: #fff3cd; border-left: 4px solid #ffc107; }}
        h1, h2, h3 {{ margin-top: 0; }}
        .timestamp {{ opacity: 0.7; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ§ª SRT GO æ¸¬è©¦å ±å‘Š</h1>
            <div class="timestamp">ç”Ÿæˆæ™‚é–“: {data['timestamp']}</div>
            <div class="timestamp">ç‰ˆæœ¬: v{data['version']}</div>
            <div class="timestamp">ç¸½åŸ·è¡Œæ™‚é–“: {data['total_duration']:.1f} ç§’</div>
        </div>
        
        <div class="summary">
            <div class="metric success">
                <h3>é€šéæ¸¬è©¦</h3>
                <div style="font-size: 2em; font-weight: bold;">{data['summary']['passed']}</div>
            </div>
            <div class="metric danger">
                <h3>å¤±æ•—æ¸¬è©¦</h3>
                <div style="font-size: 2em; font-weight: bold;">{data['summary']['failed']}</div>
            </div>
            <div class="metric warning">
                <h3>è·³éæ¸¬è©¦</h3>
                <div style="font-size: 2em; font-weight: bold;">{data['summary']['skipped']}</div>
            </div>
            <div class="metric">
                <h3>æˆåŠŸç‡</h3>
                <div style="font-size: 2em; font-weight: bold;">{data['summary']['success_rate']:.1f}%</div>
            </div>
        </div>
        
        <div class="test-results">
            <h2>è©³ç´°çµæœ</h2>
            {self.generate_test_items_html(data['test_results'])}
        </div>
    </div>
</body>
</html>"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def generate_test_items_html(self, test_results: Dict) -> str:
        """ç”Ÿæˆæ¸¬è©¦é …ç›® HTML"""
        html_items = []
        
        for test_name, result in test_results.items():
            status = 'passed' if result['success'] else 'failed'
            status_icon = 'âœ…' if result['success'] else 'âŒ'
            
            html_items.append(f"""
            <div class="test-item {status}">
                <h3>{status_icon} {test_name.replace('_', ' ').title()}</h3>
                <p>åŸ·è¡Œæ™‚é–“: {result['duration']:.1f} ç§’</p>
                {f'<p>éŒ¯èª¤: {result.get("stderr", "")}</p>' if not result['success'] else ''}
            </div>
            """)
        
        return ''.join(html_items)
    
    def calculate_summary(self) -> Dict[str, Any]:
        """è¨ˆç®—æ¸¬è©¦æ‘˜è¦"""
        total = len(self.test_results)
        passed = sum(1 for r in self.test_results.values() if r.get('success', False))
        failed = sum(1 for r in self.test_results.values() if not r.get('success', True))
        skipped = total - passed - failed
        
        return {
            'total': total,
            'passed': passed,
            'failed': failed,
            'skipped': skipped,
            'success_rate': (passed / total * 100) if total > 0 else 0
        }
    
    def show_summary(self):
        """é¡¯ç¤ºæ¸¬è©¦æ‘˜è¦"""
        print_section("æ¸¬è©¦æ‘˜è¦")
        
        summary = self.calculate_summary()
        total_time = self.end_time - self.start_time if self.end_time and self.start_time else 0
        
        print_colored(f"ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.1f} ç§’", Colors.WHITE)
        print_colored(f"ç¸½æ¸¬è©¦å¥—ä»¶: {summary['total']}", Colors.WHITE)
        print_colored(f"âœ… é€šé: {summary['passed']}", Colors.GREEN)
        print_colored(f"âŒ å¤±æ•—: {summary['failed']}", Colors.RED)
        print_colored(f"â­ï¸  è·³é: {summary['skipped']}", Colors.YELLOW)
        print_colored(f"ğŸ“Š æˆåŠŸç‡: {summary['success_rate']:.1f}%", Colors.CYAN)
        
        if summary['success_rate'] >= 90:
            print_colored("ğŸ‰ æ¸¬è©¦å“è³ª: å„ªç§€", Colors.GREEN + Colors.BOLD)
        elif summary['success_rate'] >= 80:
            print_colored("ğŸ‘ æ¸¬è©¦å“è³ª: è‰¯å¥½", Colors.YELLOW + Colors.BOLD)
        else:
            print_colored("âš ï¸  æ¸¬è©¦å“è³ª: éœ€è¦æ”¹é€²", Colors.RED + Colors.BOLD)

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='SRT GO å•†ç”¨ç´šæ¸¬è©¦åŸ·è¡Œå™¨')
    
    parser.add_argument('--skip-unit', action='store_true', help='è·³éå–®å…ƒæ¸¬è©¦')
    parser.add_argument('--skip-integration', action='store_true', help='è·³éæ•´åˆæ¸¬è©¦')
    parser.add_argument('--skip-e2e', action='store_true', help='è·³é E2E æ¸¬è©¦')
    parser.add_argument('--skip-performance', action='store_true', help='è·³éæ•ˆèƒ½æ¸¬è©¦')
    parser.add_argument('--skip-security', action='store_true', help='è·³éå®‰å…¨æ¸¬è©¦')
    parser.add_argument('--continue-on-failure', action='store_true', help='æ¸¬è©¦å¤±æ•—æ™‚ç¹¼çºŒåŸ·è¡Œ')
    parser.add_argument('--report-format', choices=['json', 'html', 'all'], default='html', help='å ±å‘Šæ ¼å¼')
    parser.add_argument('--open-report', action='store_true', help='è‡ªå‹•é–‹å•Ÿå ±å‘Š')
    
    args = parser.parse_args()
    
    # å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„
    project_root = Path(__file__).parent
    
    # å»ºç«‹æ¸¬è©¦å¥—ä»¶
    test_suite = TestSuite(project_root)
    
    # åŸ·è¡Œæ¸¬è©¦
    success = test_suite.run_all_tests(args)
    
    # é–‹å•Ÿå ±å‘Š
    if args.open_report and args.report_format in ['html', 'all']:
        report_path = project_root / 'reports' / 'test_report.html'
        if report_path.exists():
            webbrowser.open(f'file://{report_path.absolute()}')
    
    # è¨­å®šé€€å‡ºç¢¼
    sys.exit(0 if success else 1)

if __name__ == '__main__':
    main()