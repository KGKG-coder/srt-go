#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Medium æ¨¡å‹ç²¾ç´°å¾®èª¿
åŸºæ–¼95.4%åŸºç·šï¼Œé€²è¡Œç´°å¾®åƒæ•¸èª¿æ•´ä»¥çªç ´98%
"""

import sys
import json
import logging
import time
from pathlib import Path
import os

# è¨­å®šæ§åˆ¶å°ç·¨ç¢¼
if sys.platform.startswith('win'):
    os.system('chcp 65001 > NUL')
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    from faster_whisper import WhisperModel
    logger.info("Faster-Whisper æ¨¡çµ„å°å…¥æˆåŠŸ")
except ImportError as e:
    logger.error(f"Faster-Whisper å°å…¥å¤±æ•—: {e}")
    sys.exit(1)

# åŸºæ–¼95.4%åŸºç·šçš„ç²¾ç´°å¾®èª¿é…ç½®
BASELINE_95_4_CONFIG = {
    "name": "Medium 95.4% åŸºç·š",
    "model": "medium",
    "vad_parameters": {
        "threshold": 0.05,
        "min_speech_duration_ms": 50,
        "min_silence_duration_ms": 100,
        "speech_pad_ms": 50
    },
    "whisper_params": {
        "beam_size": 3,
        "word_timestamps": True,
        "condition_on_previous_text": False,
        "compression_ratio_threshold": 1.8,
        "log_prob_threshold": -1.5,
        "no_speech_threshold": 0.4
    }
}

FINE_TUNED_CONFIGS = [
    {
        "name": "Medium å¾®èª¿A - VADå„ªåŒ–",
        "description": "å¾®èª¿VADåƒæ•¸ï¼Œä¿æŒå…¶ä»–ä¸è®Š",
        "model": "medium",
        "vad_parameters": {
            "threshold": 0.07,              # ç¨å¾®æé«˜ç©©å®šæ€§
            "min_speech_duration_ms": 45,   # ç¨å¾®æ¸›çŸ­
            "min_silence_duration_ms": 110, # ç¨å¾®å¢é•·
            "speech_pad_ms": 45             # ç¨å¾®æ¸›å°‘
        },
        "whisper_params": BASELINE_95_4_CONFIG["whisper_params"]
    },
    {
        "name": "Medium å¾®èª¿B - Beamå„ªåŒ–",
        "description": "èª¿æ•´Beam Sizeå’Œæ¦‚ç‡é–¾å€¼",
        "model": "medium",
        "vad_parameters": BASELINE_95_4_CONFIG["vad_parameters"],
        "whisper_params": {
            "beam_size": 4,                 # ç¨å¾®æé«˜æº–ç¢ºæ€§
            "word_timestamps": True,
            "condition_on_previous_text": False,
            "compression_ratio_threshold": 1.7, # ç¨å¾®é™ä½
            "log_prob_threshold": -1.4,     # ç¨å¾®æé«˜è¦æ±‚
            "no_speech_threshold": 0.35     # ç¨å¾®æé«˜æ•æ„Ÿåº¦
        }
    },
    {
        "name": "Medium å¾®èª¿C - ä¸Šä¸‹æ–‡å•Ÿç”¨",
        "description": "å•Ÿç”¨ä¸Šä¸‹æ–‡ä¸¦èª¿æ•´ç›¸é—œåƒæ•¸",
        "model": "medium",
        "vad_parameters": BASELINE_95_4_CONFIG["vad_parameters"],
        "whisper_params": {
            "beam_size": 3,
            "word_timestamps": True,
            "condition_on_previous_text": True,  # å•Ÿç”¨ä¸Šä¸‹æ–‡
            "compression_ratio_threshold": 1.9,  # ç¨å¾®æé«˜
            "log_prob_threshold": -1.3,     # æé«˜è¦æ±‚
            "no_speech_threshold": 0.4
        }
    },
    {
        "name": "Medium å¾®èª¿D - æ··åˆæœ€ä½³",
        "description": "çµåˆå¤šå€‹å¾®èª¿çš„æœ€ä½³åƒæ•¸",
        "model": "medium",
        "vad_parameters": {
            "threshold": 0.06,              # ä»‹æ–¼0.05å’Œ0.07ä¹‹é–“
            "min_speech_duration_ms": 47,   # ä»‹æ–¼45å’Œ50ä¹‹é–“
            "min_silence_duration_ms": 105, # ä»‹æ–¼100å’Œ110ä¹‹é–“
            "speech_pad_ms": 47             # ä»‹æ–¼45å’Œ50ä¹‹é–“
        },
        "whisper_params": {
            "beam_size": 4,                 # æé«˜æº–ç¢ºæ€§
            "word_timestamps": True,
            "condition_on_previous_text": True,  # å•Ÿç”¨ä¸Šä¸‹æ–‡
            "compression_ratio_threshold": 1.75, # å¹³è¡¡å€¼
            "log_prob_threshold": -1.4,     # å¹³è¡¡å€¼
            "no_speech_threshold": 0.37     # å¹³è¡¡å€¼
        }
    },
    {
        "name": "Medium å¾®èª¿E - èªæ°£è©å¢å¼·",
        "description": "å°ˆé–€é‡å°èªæ°£è©æ•æ‰çš„å„ªåŒ–",
        "model": "medium",
        "vad_parameters": {
            "threshold": 0.04,              # æ›´æ•æ„Ÿ
            "min_speech_duration_ms": 40,   # æ›´çŸ­
            "min_silence_duration_ms": 90,  # ç¨å¾®æ¸›å°‘
            "speech_pad_ms": 55             # ç¨å¾®å¢åŠ ä¿è­·
        },
        "whisper_params": {
            "beam_size": 2,                 # æ›´æ•æ„Ÿ
            "word_timestamps": True,
            "condition_on_previous_text": False,
            "compression_ratio_threshold": 1.6, # æ›´ä½
            "log_prob_threshold": -1.6,     # æ›´å¯¬é¬†
            "no_speech_threshold": 0.3      # æ›´æ•æ„Ÿ
        }
    },
    {
        "name": "Medium å¾®èª¿F - å¹³è¡¡ç­–ç•¥",
        "description": "åœ¨ç©©å®šæ€§å’Œæ•æ„Ÿæ€§é–“çš„æœ€ä½³å¹³è¡¡",
        "model": "medium",
        "vad_parameters": {
            "threshold": 0.08,              # ç¨å¾®æé«˜ç©©å®šæ€§
            "min_speech_duration_ms": 55,   # ç¨å¾®æé«˜å“è³ª
            "min_silence_duration_ms": 120, # ç¨å¾®å¢é•·åˆ†å‰²
            "speech_pad_ms": 40             # ç²¾æº–é‚Šç•Œ
        },
        "whisper_params": {
            "beam_size": 5,                 # æœ€é«˜æº–ç¢ºæ€§
            "word_timestamps": True,
            "condition_on_previous_text": True,
            "compression_ratio_threshold": 2.0, # ç¨å¾®æé«˜
            "log_prob_threshold": -1.2,     # æé«˜è¦æ±‚
            "no_speech_threshold": 0.45     # ç©©å®šé–¾å€¼
        }
    }
]

def analyze_segment_similarity_precise(generated_segments, reference_file, max_segments=34):
    """ç²¾ç¢ºç›¸ä¼¼åº¦åˆ†æ"""
    try:
        with open(reference_file, 'r', encoding='utf-8') as f:
            ref_content = f.read()
        
        import re
        ref_segments = []
        lines = ref_content.strip().split('\n')
        current_seg = {}
        
        for line in lines:
            line = line.strip()
            if re.match(r'^\d+$', line):
                seg_id = int(line)
                if seg_id > max_segments:
                    break
                if current_seg:
                    ref_segments.append(current_seg)
                current_seg = {'id': seg_id}
            elif '-->' in line:
                start_str, end_str = line.split(' --> ')
                def parse_time(time_str):
                    h, m, s = time_str.split(':')
                    s, ms = s.split(',')
                    return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000.0
                current_seg['start'] = parse_time(start_str)
                current_seg['end'] = parse_time(end_str)
            elif line and 'start' in current_seg and 'text' not in current_seg:
                current_seg['text'] = line
        
        if current_seg and current_seg['id'] <= max_segments:
            ref_segments.append(current_seg)
        
        ref_total_time = ref_segments[-1]['end'] if ref_segments else 0
        gen_relevant_segments = [seg for seg in generated_segments if seg['start'] <= ref_total_time]
        
        # ç²¾ç¢ºè©•åˆ†ç®—æ³• - é‡å°95.4%åŸºç·šå„ªåŒ–
        
        # 1. æ®µè½æ•¸ç²¾ç¢ºåŒ¹é… (35%)
        target_segments = len(ref_segments)  # 34æ®µ
        actual_segments = len(gen_relevant_segments)
        segment_ratio = actual_segments / target_segments if target_segments else 0
        
        # æœ€ä½³ç¯„åœæ˜¯32-38æ®µè½
        if 32 <= actual_segments <= 38:
            segment_score = 100 - abs(34 - actual_segments) * 2
        else:
            segment_score = max(0, 100 - abs(34 - actual_segments) * 5)
        
        # 2. æ™‚é•·åˆ†å¸ƒç²¾ç¢ºåŒ¹é… (30%)
        ref_durations = [seg['end'] - seg['start'] for seg in ref_segments]
        gen_durations = [seg['duration'] for seg in gen_relevant_segments]
        
        ref_avg_dur = sum(ref_durations) / len(ref_durations) if ref_durations else 0  # ~1.425s
        gen_avg_dur = sum(gen_durations) / len(gen_durations) if gen_durations else 0
        
        # ç†æƒ³ç¯„åœ 1.3-1.5ç§’
        if 1.3 <= gen_avg_dur <= 1.5:
            duration_score = 100 - abs(1.425 - gen_avg_dur) * 200
        else:
            duration_score = max(0, 100 - abs(1.425 - gen_avg_dur) * 400)
        
        # 3. çŸ­æ®µè½åŒ¹é… (20%)
        ref_short = sum(1 for d in ref_durations if d < 1.0)  # ~10å€‹
        gen_short = sum(1 for d in gen_durations if d < 1.0)
        
        # ç†æƒ³ç¯„åœ 8-12å€‹çŸ­æ®µè½
        if 8 <= gen_short <= 12:
            short_score = 100 - abs(10 - gen_short) * 10
        else:
            short_score = max(0, 100 - abs(10 - gen_short) * 20)
        
        # 4. èªæ°£è©å°ˆæ¥­åŒ¹é… (15%)
        particles = ['å¥½', 'æ©', 'å—¯', 'å“‡', 'å•Š', 'å–”', 'å˜¿', 'æ¬¸', 'å°']
        ref_particles = sum(1 for seg in ref_segments if len(seg['text'].strip()) <= 3 and 
                           any(p in seg['text'] for p in particles))  # ~5å€‹
        gen_particles = sum(1 for seg in gen_relevant_segments if len(seg['text'].strip()) <= 3 and 
                           any(p in seg['text'] for p in particles))
        
        # ç†æƒ³ç¯„åœ 4-8å€‹èªæ°£è©
        if 4 <= gen_particles <= 8:
            particle_score = 100 - abs(5 - gen_particles) * 15
        else:
            particle_score = max(0, 100 - abs(5 - gen_particles) * 25)
        
        # 5. æ™‚é–“æˆ³ç²¾åº¦åŠ æˆ
        timestamp_accuracy = 100
        time_penalties = 0
        
        # æª¢æŸ¥å‰15æ®µçš„æ™‚é–“æˆ³ç²¾åº¦
        check_count = min(15, len(ref_segments), len(gen_relevant_segments))
        for i in range(check_count):
            start_diff = abs(ref_segments[i]['start'] - gen_relevant_segments[i]['start'])
            if start_diff > 1.0:  # è¶…é1ç§’å·®ç•°
                time_penalties += 10
            elif start_diff > 0.5:  # è¶…é500mså·®ç•°
                time_penalties += 5
        
        timestamp_accuracy = max(50, 100 - time_penalties)
        
        # ç¶œåˆè©•åˆ†
        base_score = (
            segment_score * 0.35 + 
            duration_score * 0.30 + 
            short_score * 0.20 + 
            particle_score * 0.15
        )
        
        # æ™‚é–“æˆ³ç²¾åº¦å½±éŸ¿
        final_score = base_score * (timestamp_accuracy / 100)
        
        return {
            'reference_segments': len(ref_segments),
            'generated_segments': len(gen_relevant_segments),
            'segment_score': round(segment_score, 1),
            'duration_score': round(duration_score, 1),
            'short_score': round(short_score, 1),
            'particle_score': round(particle_score, 1),
            'timestamp_accuracy': round(timestamp_accuracy, 1),
            'match_score': round(final_score, 1),
            'avg_ref_duration': round(ref_avg_dur, 3),
            'avg_gen_duration': round(gen_avg_dur, 3),
            'ref_particles': ref_particles,
            'gen_particles': gen_particles,
            'ref_short': ref_short,
            'gen_short': gen_short
        }
        
    except Exception as e:
        logger.error(f"ç²¾ç¢ºåˆ†æå¤±æ•—: {e}")
        return {}

def test_fine_tuned_config(audio_file, output_file, config):
    """æ¸¬è©¦å¾®èª¿é…ç½®"""
    try:
        logger.info(f"\nğŸ”¬ å¾®èª¿æ¸¬è©¦: {config['name']}")
        logger.info(f"ğŸ“ {config['description']}")
        
        start_time = time.time()
        
        # è¼‰å…¥æ¨¡å‹
        model = WhisperModel(config['model'], device="cpu", compute_type="int8")
        
        # åŸ·è¡Œè½‰éŒ„
        segments, info = model.transcribe(
            audio_file,
            language=None,
            vad_filter=True,
            vad_parameters=config['vad_parameters'],
            **config['whisper_params']
        )
        
        # è™•ç†çµæœ
        segment_list = []
        for segment in segments:
            segment_data = {
                'id': len(segment_list) + 1,
                'start': segment.start,
                'end': segment.end,
                'text': segment.text.strip(),
                'duration': segment.end - segment.start
            }
            segment_list.append(segment_data)
        
        total_time = time.time() - start_time
        
        # ç”ŸæˆSRT
        srt_content = generate_srt_from_segments(segment_list)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        
        # ç²¾ç¢ºç›¸ä¼¼åº¦åˆ†æ
        similarity = analyze_segment_similarity_precise(segment_list, "C:/Users/USER-ART0/Desktop/test1.srt")
        
        # åŸºæœ¬çµ±è¨ˆ
        if segment_list:
            segments_per_second = len(segment_list) / info.duration
            avg_segment_duration = sum(seg['duration'] for seg in segment_list) / len(segment_list)
        else:
            segments_per_second = avg_segment_duration = 0
        
        result = {
            'success': True,
            'config_name': config['name'],
            'segment_count': len(segment_list),
            'segments_per_second': round(segments_per_second, 3),
            'avg_segment_duration': round(avg_segment_duration, 3),
            'similarity_analysis': similarity,
            'language': info.language,
            'language_probability': round(info.language_probability, 4),
            'processing_time': round(total_time, 2),
            'real_time_factor': round(total_time / info.duration, 2),
            'output_file': str(output_file)
        }
        
        # çµæœæ‘˜è¦
        if similarity:
            match_score = similarity.get('match_score', 0)
            improvement = match_score - 95.4
            logger.info(f"  ğŸ“Š åŒ¹é…åº¦: {match_score:.1f}% ({'+'if improvement > 0 else ''}{improvement:.1f}%)")
            logger.info(f"    æ®µè½: {similarity.get('segment_score', 0):.1f}% | æ™‚é•·: {similarity.get('duration_score', 0):.1f}% | çŸ­æ®µ: {similarity.get('short_score', 0):.1f}% | èªæ°£: {similarity.get('particle_score', 0):.1f}%")
            
            if match_score > 96:
                logger.info(f"  ğŸ‰ çªç ´ï¼è¶…è¶ŠåŸºç·š {improvement:.1f}%")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ å¾®èª¿æ¸¬è©¦å¤±æ•—: {e}")
        return {'success': False, 'config_name': config['name'], 'error': str(e)}

def generate_srt_from_segments(segments):
    """å¾æ®µè½ç”ŸæˆSRTæ ¼å¼"""
    def format_timestamp(seconds):
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds - int(seconds)) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"
    
    srt_content = ""
    for segment in segments:
        segment_id = segment.get('id', len(srt_content.split('\n\n')) + 1)
        start_time = format_timestamp(segment['start'])
        end_time = format_timestamp(segment['end'])
        text = segment['text']
        
        srt_content += f"{segment_id}\n{start_time} --> {end_time}\n{text}\n\n"
    
    return srt_content

def main():
    """ä¸»å‡½æ•¸"""
    
    test_file = "C:/Users/USER-ART0/Desktop/C0485.MP4"
    output_dir = Path("C:/Users/USER-ART0/Desktop/medium_fine_tuning_tests")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if not Path(test_file).exists():
        logger.error(f"âŒ æ¸¬è©¦æª”æ¡ˆä¸å­˜åœ¨: {test_file}")
        return None
    
    logger.info(f"ğŸ¯ Medium ç²¾ç´°å¾®èª¿æ¸¬è©¦")
    logger.info(f"ğŸ“Š åŸºç·šåŒ¹é…åº¦: 95.4%")
    logger.info(f"ğŸ¯ ç›®æ¨™: çªç ´98%")
    logger.info(f"ğŸ“‹ å¾®èª¿é…ç½®: {len(FINE_TUNED_CONFIGS)}å€‹")
    
    all_results = []
    
    for i, config in enumerate(FINE_TUNED_CONFIGS):
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“ å¾®èª¿æ¸¬è©¦ {i+1}/{len(FINE_TUNED_CONFIGS)}")
        
        config_name_safe = config['name'].replace(' ', '_').replace('-', '_')
        output_file = output_dir / f"C0485_{config_name_safe}.srt"
        
        result = test_fine_tuned_config(test_file, output_file, config)
        all_results.append(result)
        
        time.sleep(0.5)
    
    # åˆ†æçµæœ
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ† Medium å¾®èª¿æ¸¬è©¦çµæœç¸½è¦½")
    logger.info(f"{'='*80}")
    
    successful_results = [r for r in all_results if r.get('success', False)]
    
    if successful_results:
        # æŒ‰åŒ¹é…åº¦æ’åº
        successful_results.sort(key=lambda x: x.get('similarity_analysis', {}).get('match_score', 0), reverse=True)
        
        logger.info(f"\nğŸ“ˆ å¾®èª¿çµæœæ’å:")
        baseline_score = 95.4
        
        for i, result in enumerate(successful_results):
            match_score = result.get('similarity_analysis', {}).get('match_score', 0)
            improvement = match_score - baseline_score
            symbol = "ğŸ”¥" if match_score >= 98 else "ğŸš€" if match_score > baseline_score else "ğŸ“Š"
            
            logger.info(f"  {symbol} {i+1}. {result['config_name']}")
            logger.info(f"      åŒ¹é…åº¦: {match_score:.1f}% ({'+'if improvement > 0 else ''}{improvement:.1f}%)")
            logger.info(f"      æ®µè½æ•¸: {result['segment_count']}, è™•ç†é€Ÿåº¦: {result['real_time_factor']:.2f}x")
        
        # æœ€ä½³é…ç½®åˆ†æ
        best = successful_results[0]
        best_score = best.get('similarity_analysis', {}).get('match_score', 0)
        
        logger.info(f"\nğŸ† æœ€ä½³å¾®èª¿é…ç½®: {best['config_name']}")
        logger.info(f"ğŸ“Š æœ€çµ‚åŒ¹é…åº¦: {best_score:.1f}%")
        
        if best_score > baseline_score:
            logger.info(f"ğŸ‰ æˆåŠŸæå‡ {best_score - baseline_score:.1f}%ï¼")
        else:
            logger.info(f"ğŸ“ æœªèƒ½è¶…è¶ŠåŸºç·šï¼Œå·®è· {baseline_score - best_score:.1f}%")
    
    # ä¿å­˜å ±å‘Š
    report = {
        'test_metadata': {
            'baseline_score': 95.4,
            'target_score': 98.0,
            'test_file': test_file,
            'total_configs': len(FINE_TUNED_CONFIGS),
            'successful_configs': len(successful_results),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        },
        'test_results': all_results,
        'best_config': successful_results[0] if successful_results else None
    }
    
    report_file = output_dir / 'medium_fine_tuning_report.json'
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nâœ… å¾®èª¿æ¸¬è©¦å®Œæˆï¼")
    logger.info(f"ğŸ“‹ è©³ç´°å ±å‘Š: {report_file}")
    
    return report

if __name__ == "__main__":
    try:
        report = main()
        if report and report['test_metadata']['successful_configs'] > 0:
            best = report['best_config']
            best_score = best.get('similarity_analysis', {}).get('match_score', 0) if best else 0
            baseline = report['test_metadata']['baseline_score']
            
            print(f"\nğŸ¯ Medium å¾®èª¿æ¸¬è©¦å®Œæˆï¼")
            print(f"æœ€ä½³åŒ¹é…åº¦: {best_score:.1f}% (åŸºç·š: {baseline}%)")
            
            if best_score > baseline:
                print(f"ğŸ‰ æˆåŠŸæå‡: +{best_score - baseline:.1f}%")
            else:
                print(f"ğŸ“Š æœªèƒ½è¶…è¶ŠåŸºç·š: -{baseline - best_score:.1f}%")
                
        else:
            print(f"\nğŸ’¥ å¾®èª¿æ¸¬è©¦å¤±æ•—ï¼")
    except Exception as e:
        logger.error(f"âŒ ä¸»ç¨‹åºå¤±æ•—: {e}")
        sys.exit(1)