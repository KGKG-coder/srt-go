#!/usr/bin/env python3
"""
æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨
æ··åˆç­–ç•¥ï¼šé ç½®åŸºç¤æ¨¡å‹ + æ™ºèƒ½ä¸‹è¼‰ + å‚™ç”¨æ–¹æ¡ˆ
"""

import os
import sys
import json
import time
import shutil
import logging
import tempfile
import urllib.request
from pathlib import Path
from typing import Optional, Tuple, Dict

logger = logging.getLogger(__name__)


class SmartModelManager:
    """æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        # è·¯å¾‘é…ç½®
        if hasattr(sys, '_MEIPASS'):  # PyInstaller ç’°å¢ƒ
            self.app_models_dir = Path(sys._MEIPASS) / "models"
        else:
            self.app_models_dir = Path(__file__).parent / "models"
        
        self.cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
        
        # æ¨¡å‹é…ç½®
        self.model_config = {
            "base": {
                "size_mb": 142,
                "bundled": True,  # å…§å»ºåœ¨å®‰è£ç¨‹å¼ä¸­
                "fallback_priority": 1,
                "description": "å¿«é€Ÿæ¨¡å‹ï¼Œé›¢ç·šå¯ç”¨"
            },
            "medium": {
                "size_mb": 1500,
                "bundled": True,  # å…§å»ºåœ¨å®‰è£ç¨‹å¼ä¸­
                "fallback_priority": 2,
                "description": "æ¨è–¦æ¨¡å‹ï¼Œé›¢ç·šå¯ç”¨"
            },
            "large": {
                "size_mb": 5800,
                "bundled": False,  # éœ€è¦ä¸‹è¼‰
                "fallback_priority": 3,
                "description": "æœ€é«˜ç²¾åº¦ï¼Œéœ€è¦ç¶²è·¯ä¸‹è¼‰"
            }
        }
        
        # ä¸‹è¼‰æºé…ç½®
        self.download_sources = [
            "https://huggingface.co/Systran/faster-whisper-{model}",
            # å¯ä»¥æ·»åŠ å‚™ç”¨ä¸‹è¼‰æº
        ]
    
    def check_network_connectivity(self) -> bool:
        """æª¢æŸ¥ç¶²è·¯é€£æ¥"""
        test_urls = [
            "https://huggingface.co",
            "https://github.com",
            "https://www.google.com"
        ]
        
        for url in test_urls:
            try:
                urllib.request.urlopen(url, timeout=5)
                logger.info(f"ç¶²è·¯é€£æ¥æ­£å¸¸: {url}")
                return True
            except:
                continue
        
        logger.warning("ç¶²è·¯é€£æ¥ä¸å¯ç”¨")
        return False
    
    def get_model_availability(self, model_name: str) -> Dict[str, any]:
        """ç²å–æ¨¡å‹å¯ç”¨æ€§ç‹€æ…‹"""
        config = self.model_config.get(model_name, {})
        status = {
            "model": model_name,
            "bundled": config.get("bundled", False),
            "cached": False,
            "downloadable": False,
            "available": False,
            "source": None,
            "path": None,
            "fallback_options": []
        }
        
        # æª¢æŸ¥å…§å»ºæ¨¡å‹
        if config.get("bundled"):
            bundled_path = self.app_models_dir / f"whisper-{model_name}-model.zip"
            if bundled_path.exists():
                status["available"] = True
                status["source"] = "bundled"
                status["path"] = str(bundled_path.parent)
                logger.info(f"ç™¼ç¾å…§å»ºæ¨¡å‹: {model_name}")
        
        # æª¢æŸ¥ç·©å­˜æ¨¡å‹
        cached_paths = [
            self.cache_dir / f"models--Systran--faster-whisper-{model_name}",
            self.cache_dir / f"models--openai--whisper-{model_name}"
        ]
        
        for cached_path in cached_paths:
            if cached_path.exists():
                status["cached"] = True
                if not status["available"]:  # å„ªå…ˆä½¿ç”¨å…§å»º
                    status["available"] = True
                    status["source"] = "cached"
                    status["path"] = str(cached_path)
                logger.info(f"ç™¼ç¾ç·©å­˜æ¨¡å‹: {model_name} at {cached_path}")
                break
        
        # æª¢æŸ¥ä¸‹è¼‰å¯èƒ½æ€§
        if not status["available"] and self.check_network_connectivity():
            status["downloadable"] = True
            logger.info(f"æ¨¡å‹ {model_name} å¯é€šéç¶²è·¯ä¸‹è¼‰")
        
        # è¨­ç½®å‚™ç”¨é¸é …
        if not status["available"]:
            for alt_name, alt_config in self.model_config.items():
                if (alt_name != model_name and 
                    alt_config.get("bundled") and 
                    alt_config.get("fallback_priority", 99) < config.get("fallback_priority", 99)):
                    alt_bundled = self.app_models_dir / f"whisper-{alt_name}-model.zip"
                    if alt_bundled.exists():
                        status["fallback_options"].append({
                            "model": alt_name,
                            "reason": f"è‡ªå‹•é™ç´šåˆ°å¯ç”¨çš„ {alt_name} æ¨¡å‹"
                        })
        
        return status
    
    def get_best_available_model(self, requested_model: str, progress_callback=None) -> Tuple[bool, str, Optional[str]]:
        """ç²å–æœ€ä½³å¯ç”¨æ¨¡å‹"""
        try:
            if progress_callback:
                progress_callback(5, f"æª¢æŸ¥ {requested_model} æ¨¡å‹...")
            
            status = self.get_model_availability(requested_model)
            
            # å¦‚æœè«‹æ±‚çš„æ¨¡å‹å¯ç”¨
            if status["available"]:
                if progress_callback:
                    source_msg = {
                        "bundled": "ä½¿ç”¨å…§å»ºæ¨¡å‹",
                        "cached": "ä½¿ç”¨ç·©å­˜æ¨¡å‹"
                    }.get(status["source"], "æ¨¡å‹å·²å°±ç·’")
                    progress_callback(20, f"{requested_model}: {source_msg}")
                
                return True, requested_model, status["path"]
            
            # å¦‚æœå¯ä»¥ä¸‹è¼‰
            if status["downloadable"]:
                if progress_callback:
                    progress_callback(10, f"æ­£åœ¨ä¸‹è¼‰ {requested_model} æ¨¡å‹...")
                
                # è®“ faster-whisper è‡ªå·±è™•ç†ä¸‹è¼‰
                logger.info(f"å°‡ä½¿ç”¨è‡ªå‹•ä¸‹è¼‰: {requested_model}")
                return True, requested_model, None  # None è®“ faster-whisper è‡ªå‹•ä¸‹è¼‰
            
            # ä½¿ç”¨å‚™ç”¨æ¨¡å‹
            if status["fallback_options"]:
                fallback = status["fallback_options"][0]
                fallback_model = fallback["model"]
                
                if progress_callback:
                    progress_callback(15, f"é™ç´šåˆ° {fallback_model} æ¨¡å‹...")
                
                fallback_status = self.get_model_availability(fallback_model)
                if fallback_status["available"]:
                    logger.warning(f"ä½¿ç”¨å‚™ç”¨æ¨¡å‹: {fallback_model} (åŸå› : {fallback['reason']})")
                    return True, fallback_model, fallback_status["path"]
            
            # å®Œå…¨å¤±æ•—
            logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ {requested_model}ï¼Œä¹Ÿç„¡å¯ç”¨å‚™ç”¨æ–¹æ¡ˆ")
            return False, requested_model, None
            
        except Exception as e:
            logger.error(f"ç²å–æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False, requested_model, None
    
    def get_model_info_for_ui(self) -> Dict[str, Dict]:
        """ç‚ºUIæä¾›æ¨¡å‹è³‡è¨Š"""
        model_info = {}
        
        for model_name, config in self.model_config.items():
            status = self.get_model_availability(model_name)
            
            model_info[model_name] = {
                "name": model_name.capitalize(),
                "size_mb": config["size_mb"],
                "description": config["description"],
                "available": status["available"],
                "source": status["source"],
                "bundled": config["bundled"],
                "downloadable": status["downloadable"],
                "status_text": self._get_status_text(status)
            }
        
        return model_info
    
    def _get_status_text(self, status: Dict) -> str:
        """ç²å–ç‹€æ…‹é¡¯ç¤ºæ–‡å­—"""
        if status["available"]:
            if status["source"] == "bundled":
                return "âœ… å·²å®‰è£"
            elif status["source"] == "cached":
                return "ğŸ’¾ å·²ç·©å­˜"
            else:
                return "âœ… å¯ç”¨"
        elif status["downloadable"]:
            return "â¬‡ï¸ éœ€è¦ä¸‹è¼‰"
        else:
            return "âŒ ä¸å¯ç”¨"
    
    def cleanup_cache(self, max_age_days: int = 30):
        """æ¸…ç†èˆŠç·©å­˜"""
        try:
            if not self.cache_dir.exists():
                return
            
            current_time = time.time()
            max_age_seconds = max_age_days * 24 * 3600
            cleaned_count = 0
            
            for item in self.cache_dir.iterdir():
                if item.is_dir() and "whisper" in item.name.lower():
                    # æª¢æŸ¥ç›®éŒ„ä¿®æ”¹æ™‚é–“
                    age = current_time - item.stat().st_mtime
                    if age > max_age_seconds:
                        try:
                            shutil.rmtree(item)
                            cleaned_count += 1
                            logger.info(f"æ¸…ç†èˆŠç·©å­˜: {item.name}")
                        except Exception as e:
                            logger.warning(f"æ¸…ç†ç·©å­˜å¤±æ•—: {item.name}, {e}")
            
            if cleaned_count > 0:
                logger.info(f"æ¸…ç†äº† {cleaned_count} å€‹èˆŠç·©å­˜ç›®éŒ„")
                
        except Exception as e:
            logger.error(f"ç·©å­˜æ¸…ç†å¤±æ•—: {e}")


def test_smart_manager():
    """æ¸¬è©¦æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨"""
    manager = SmartModelManager()
    
    print("=== æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨æ¸¬è©¦ ===")
    
    # æª¢æŸ¥ç¶²è·¯é€£æ¥
    network_ok = manager.check_network_connectivity()
    print(f"ç¶²è·¯é€£æ¥: {'âœ… æ­£å¸¸' if network_ok else 'âŒ ä¸å¯ç”¨'}")
    
    # æª¢æŸ¥æ‰€æœ‰æ¨¡å‹ç‹€æ…‹
    print("\n--- æ¨¡å‹ç‹€æ…‹æª¢æŸ¥ ---")
    for model_name in ["base", "medium", "large"]:
        status = manager.get_model_availability(model_name)
        print(f"{model_name:6}: {manager._get_status_text(status):12} | æº: {status.get('source', 'none'):8} | å‚™ç”¨: {len(status['fallback_options'])}")
    
    # æ¸¬è©¦æœ€ä½³æ¨¡å‹é¸æ“‡
    print("\n--- æœ€ä½³æ¨¡å‹é¸æ“‡æ¸¬è©¦ ---")
    for model_name in ["base", "medium", "large"]:
        def progress(percent, message):
            print(f"  [{percent:3d}%] {message}")
        
        success, actual_model, path = manager.get_best_available_model(model_name, progress)
        result = f"âœ… {actual_model}" if success else "âŒ å¤±æ•—"
        print(f"{model_name} -> {result}")
    
    # UI æ¨¡å‹è³‡è¨Š
    print("\n--- UI æ¨¡å‹è³‡è¨Š ---")
    ui_info = manager.get_model_info_for_ui()
    for model, info in ui_info.items():
        print(f"{model}: {info['status_text']} - {info['description']}")


if __name__ == "__main__":
    test_smart_manager()