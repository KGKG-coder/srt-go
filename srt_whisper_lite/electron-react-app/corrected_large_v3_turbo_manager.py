#!/usr/bin/env python3
"""
ä¿®æ­£çš„ Large V3 Turbo æ¨¡å‹ç®¡ç†å™¨
è§£æ±ºæ¨¡å‹å¤§å°ä¸ç¬¦å’Œç¼ºå¤±æª”æ¡ˆå•é¡Œ
"""

import os
import logging
import time
from pathlib import Path
from typing import Tuple, Dict, Any, Optional

logger = logging.getLogger(__name__)

class CorrectedLargeV3TurboManager:
    """ä¿®æ­£çš„ Large V3 Turbo æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        # ä¿®æ­£çš„æ¨¡å‹å„ªå…ˆé †åº
        self.model_candidates = [
            {
                "name": "distil-whisper/distil-large-v3",
                "description": "Official Distilled Large V3",
                "expected_size_mb": 756,
                "model_bin_size_mb": 700,  # model.bin æ‡‰è©²ç´„ 700MB
                "speed_boost": "6x faster",
                "accuracy": "97% of original",
                "priority": 1
            },
            {
                "name": "Systran/faster-distil-whisper-large-v3", 
                "description": "Systran Distilled Version",
                "expected_size_mb": 800,
                "model_bin_size_mb": 750,
                "speed_boost": "5x faster", 
                "accuracy": "96% of original",
                "priority": 2
            },
            {
                "name": "Systran/faster-whisper-large-v3",
                "description": "Standard Large V3 (Fallback)",
                "expected_size_mb": 3100,
                "model_bin_size_mb": 2900,
                "speed_boost": "4x faster",
                "accuracy": "99.5% of original", 
                "priority": 3
            }
        ]
        
        # è·¯å¾‘è¨­å®š
        self.cache_base = Path.home() / ".cache" / "huggingface" / "hub"
        self.current_model = None
        self.model_status = "not_checked"
        
    def check_available_models(self) -> Dict[str, Dict]:
        """æª¢æŸ¥æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        available_models = {}
        
        for candidate in self.model_candidates:
            model_name = candidate["name"]
            cache_dir_name = f"models--{model_name.replace('/', '--')}"
            cache_path = self.cache_base / cache_dir_name
            
            status = self._validate_model_cache(cache_path, candidate)
            available_models[model_name] = {
                **candidate,
                "cache_path": cache_path,
                "status": status["status"],
                "actual_size_mb": status["actual_size_mb"],
                "missing_files": status["missing_files"],
                "usable": status["usable"]
            }
            
        return available_models
    
    def _validate_model_cache(self, cache_path: Path, candidate: Dict) -> Dict:
        """é©—è­‰æ¨¡å‹å¿«å–çš„å®Œæ•´æ€§"""
        if not cache_path.exists():
            return {
                "status": "not_downloaded",
                "actual_size_mb": 0,
                "missing_files": ["entire_model"],
                "usable": False
            }
        
        # å°‹æ‰¾ snapshots ç›®éŒ„
        snapshots_dir = cache_path / "snapshots"
        if not snapshots_dir.exists():
            return {
                "status": "incomplete_download",
                "actual_size_mb": 0,
                "missing_files": ["snapshots"],
                "usable": False
            }
        
        # æª¢æŸ¥æœ€æ–°çš„ snapshot
        snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
        if not snapshot_dirs:
            return {
                "status": "no_snapshots",
                "actual_size_mb": 0,
                "missing_files": ["snapshot_data"],
                "usable": False
            }
        
        # ä½¿ç”¨æœ€æ–°çš„ snapshot
        latest_snapshot = max(snapshot_dirs, key=lambda d: d.stat().st_mtime)
        
        # æª¢æŸ¥å¿…è¦æª”æ¡ˆ
        required_files = ["config.json", "model.bin", "tokenizer.json"]
        missing_files = []
        actual_size_bytes = 0
        
        for req_file in required_files:
            file_path = latest_snapshot / req_file
            if file_path.exists():
                actual_size_bytes += file_path.stat().st_size
            else:
                missing_files.append(req_file)
        
        actual_size_mb = actual_size_bytes / (1024 * 1024)
        expected_size_mb = candidate["expected_size_mb"]
        
        # åˆ¤æ–·ç‹€æ…‹
        if missing_files:
            if "model.bin" in missing_files:
                status = "missing_model_bin"
            else:
                status = "missing_config_files"
            usable = False
        elif actual_size_mb < expected_size_mb * 0.8:  # å…è¨± 20% èª¤å·®
            status = "size_mismatch" 
            usable = False
        else:
            status = "complete"
            usable = True
            
        return {
            "status": status,
            "actual_size_mb": round(actual_size_mb, 1),
            "missing_files": missing_files,
            "usable": usable,
            "snapshot_path": latest_snapshot
        }
    
    def get_best_available_model(self) -> Tuple[str, Dict]:
        """ç²å–æœ€ä½³å¯ç”¨æ¨¡å‹"""
        available_models = self.check_available_models()
        
        # æŒ‰å„ªå…ˆé †åºæ’åºä¸¦å°‹æ‰¾å¯ç”¨çš„æ¨¡å‹
        sorted_models = sorted(
            available_models.items(),
            key=lambda item: item[1]["priority"]
        )
        
        for model_name, model_info in sorted_models:
            if model_info["usable"]:
                logger.info(f"âœ… é¸æ“‡æ¨¡å‹: {model_name} ({model_info['actual_size_mb']}MB)")
                return model_name, model_info
        
        # å¦‚æœæ²’æœ‰å®Œå…¨å¯ç”¨çš„æ¨¡å‹ï¼Œå›å ±ç‹€æ³
        logger.warning("âš ï¸ æ²’æœ‰å®Œå…¨å¯ç”¨çš„æ¨¡å‹ï¼Œåˆ†æå•é¡Œ:")
        for model_name, model_info in sorted_models:
            logger.warning(f"  {model_name}: {model_info['status']} (ç¼ºå¤±: {model_info['missing_files']})")
        
        # è¿”å›æœ€é«˜å„ªå…ˆç´šçš„æ¨¡å‹ï¼ˆå³ä½¿ä¸å®Œæ•´ï¼‰
        fallback_name, fallback_info = sorted_models[0]
        logger.info(f"ğŸ”„ ä½¿ç”¨å¾Œå‚™æ¨¡å‹: {fallback_name}")
        return fallback_name, fallback_info
    
    def ensure_model_ready(self) -> Tuple[bool, str, Dict]:
        """ç¢ºä¿æ¨¡å‹æº–å‚™å°±ç·’"""
        model_name, model_info = self.get_best_available_model()
        
        if model_info["usable"]:
            # è½‰æ›ç‚º faster-whisper å¯è­˜åˆ¥çš„åç¨±
            whisper_model_name = self._convert_to_whisper_name(model_name)
            return True, whisper_model_name, model_info
        else:
            # æ¨¡å‹ä¸å¯ç”¨ï¼Œéœ€è¦è™•ç†
            return False, model_name, model_info
    
    def _convert_to_whisper_name(self, hf_model_name: str) -> str:
        """å°‡ HuggingFace æ¨¡å‹åç¨±è½‰æ›ç‚º faster-whisper è­˜åˆ¥çš„åç¨±"""
        name_mapping = {
            "distil-whisper/distil-large-v3": "distil-large-v3",
            "Systran/faster-distil-whisper-large-v3": "distil-large-v3",  # ä¹Ÿä½¿ç”¨ distil åç¨±
            "Systran/faster-whisper-large-v3": "large-v3"
        }
        return name_mapping.get(hf_model_name, "large-v3")
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹è©³ç´°ä¿¡æ¯"""
        success, model_name, model_info = self.ensure_model_ready()
        
        return {
            "success": success,
            "model_name": model_name,
            "status": model_info["status"],
            "actual_size": f"{model_info['actual_size_mb']} MB",
            "expected_size": f"{model_info['expected_size_mb']} MB",
            "missing_files": model_info["missing_files"],
            "usable": model_info["usable"],
            "description": model_info["description"],
            "performance": model_info["speed_boost"],
            "accuracy": model_info["accuracy"]
        }
    
    def get_whisper_model_params(self) -> Dict[str, Any]:
        """ç²å– faster-whisper æ¨¡å‹åƒæ•¸"""
        success, model_name, model_info = self.ensure_model_ready()
        
        if not success:
            logger.warning(f"æ¨¡å‹ä¸å®Œæ•´ï¼Œä½¿ç”¨æ¨™æº– large-v3 ä½œç‚ºå¾Œå‚™")
            model_name = "large-v3"
        
        # å„ªåŒ–çš„åƒæ•¸é…ç½®
        params = {
            "model_size_or_path": model_name,
            "device": "cuda" if self._check_cuda_available() else "cpu",
            "compute_type": "float16" if self._check_cuda_available() else "int8",
            "num_workers": 1
        }
        
        if params["device"] == "cuda":
            params["device_index"] = 0
        
        return params
    
    def _check_cuda_available(self) -> bool:
        """æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def download_missing_model(self, model_name: str) -> bool:
        """ä¸‹è¼‰ç¼ºå¤±çš„æ¨¡å‹"""
        try:
            from huggingface_hub import snapshot_download
            
            logger.info(f"é–‹å§‹ä¸‹è¼‰æ¨¡å‹: {model_name}")
            
            # ä¸‹è¼‰åˆ°å¿«å–ç›®éŒ„
            local_dir = snapshot_download(
                repo_id=model_name,
                cache_dir=self.cache_base.parent,
                resume_download=True
            )
            
            logger.info(f"âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ: {local_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¸‹è¼‰å¤±æ•—: {e}")
            return False
    
    def fix_incomplete_models(self) -> Dict[str, bool]:
        """ä¿®å¾©ä¸å®Œæ•´çš„æ¨¡å‹"""
        available_models = self.check_available_models()
        results = {}
        
        for model_name, model_info in available_models.items():
            if not model_info["usable"] and model_info["status"] != "not_downloaded":
                logger.info(f"å˜—è©¦ä¿®å¾©æ¨¡å‹: {model_name}")
                results[model_name] = self.download_missing_model(model_name)
            else:
                results[model_name] = model_info["usable"]
        
        return results

def get_corrected_turbo_manager() -> CorrectedLargeV3TurboManager:
    """ä¾¿åˆ©å‡½æ•¸ï¼šç²å–ä¿®æ­£çš„ Turbo æ¨¡å‹ç®¡ç†å™¨"""
    return CorrectedLargeV3TurboManager()

if __name__ == "__main__":
    # æ¸¬è©¦ä¿®æ­£çš„ç®¡ç†å™¨
    manager = CorrectedLargeV3TurboManager()
    
    print("=== ä¿®æ­£çš„ Large V3 Turbo ç®¡ç†å™¨æ¸¬è©¦ ===")
    print()
    
    # æª¢æŸ¥å¯ç”¨æ¨¡å‹
    print("æª¢æŸ¥å¯ç”¨æ¨¡å‹:")
    available = manager.check_available_models()
    for name, info in available.items():
        status_icon = "âœ…" if info["usable"] else "âŒ"
        print(f"  {status_icon} {name}: {info['status']} ({info['actual_size_mb']}MB)")
        if info["missing_files"]:
            print(f"      ç¼ºå¤±æª”æ¡ˆ: {info['missing_files']}")
    
    print()
    print("æœ€ä½³æ¨¡å‹é¸æ“‡:")
    model_info = manager.get_model_info()
    for key, value in model_info.items():
        print(f"  {key}: {value}")
    
    print()
    print("Whisper åƒæ•¸:")
    params = manager.get_whisper_model_params()
    for key, value in params.items():
        print(f"  {key}: {value}")