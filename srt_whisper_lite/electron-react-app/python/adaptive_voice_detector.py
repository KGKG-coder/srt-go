#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªé©æ‡‰äººè²æª¢æ¸¬å™¨
å®Œå…¨åŸºæ–¼éŸ³é »ç‰¹å¾µçš„å‹•æ…‹äººè²æª¢æ¸¬ï¼Œç„¡ç¡¬ç·¨ç¢¼é–¾å€¼
"""

import numpy as np
import logging
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import math
import scipy.signal
from scipy.cluster.vq import kmeans2, whiten
from sklearn.preprocessing import StandardScaler
import librosa

logger = logging.getLogger(__name__)


class AdaptiveVoiceDetector:
    """
    è‡ªé©æ‡‰äººè²æª¢æ¸¬å™¨
    
    åŸºæ–¼å¤šç¶­éŸ³é »ç‰¹å¾µçš„ç„¡ç¡¬ç·¨ç¢¼äººè²æª¢æ¸¬ç³»çµ±ï¼š
    - å‹•æ…‹å­¸ç¿’éŸ³é »å…§å®¹ç‰¹å¾µåˆ†ä½ˆ
    - è‡ªé©æ‡‰èšé¡å€åˆ†äººè²èˆ‡éäººè²
    - ç²¾ç¢ºçš„æ™‚é–“é‚Šç•Œå®šä½
    """
    
    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate
        self.frame_length = int(sample_rate * 0.025)  # 25ms frames
        self.hop_length = int(sample_rate * 0.010)    # 10ms hop
        
        # ç‰¹å¾µçµ±è¨ˆï¼ˆå‹•æ…‹è¨ˆç®—ï¼‰
        self.audio_features_stats = {}
        self.voice_model = None
        self.scaler = StandardScaler()
        
        # äººè²æª¢æ¸¬åƒæ•¸ï¼ˆå‹•æ…‹è¨­å®šï¼‰
        self.voice_confidence_threshold = None  # å‹•æ…‹è¨ˆç®—
        self.segment_min_voice_ratio = None     # å‹•æ…‹è¨ˆç®—
        
        logger.info("è‡ªé©æ‡‰äººè²æª¢æ¸¬å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def detect_voice_segments(self, segments: List[Dict], audio_file: str) -> List[Dict]:
        """
        æª¢æ¸¬ä¸¦ä¿®æ­£èªéŸ³æ®µè½çš„æ™‚é–“é‚Šç•Œ
        
        Args:
            segments: åŸå§‹å­—å¹•æ®µè½
            audio_file: éŸ³é »æ–‡ä»¶è·¯å¾‘
            
        Returns:
            List[Dict]: ä¿®æ­£å¾Œçš„æ®µè½ï¼ˆåƒ…åŒ…å«çœŸå¯¦äººè²éƒ¨åˆ†ï¼‰
        """
        try:
            logger.info("é–‹å§‹è‡ªé©æ‡‰äººè²æª¢æ¸¬")
            
            # 1. è¼‰å…¥éŸ³é »ä¸¦åˆ†æç‰¹å¾µ
            audio_data, sr = self._load_audio(audio_file)
            if audio_data is None:
                logger.warning("éŸ³é »è¼‰å…¥å¤±æ•—ï¼Œè·³éäººè²æª¢æ¸¬")
                return segments
            
            # 2. å…¨åŸŸç‰¹å¾µåˆ†æå’Œå­¸ç¿’
            self._analyze_global_features(audio_data, sr)
            
            # 3. ç‚ºæ¯å€‹æ®µè½é€²è¡Œäººè²æª¢æ¸¬
            voice_segments = self._process_segments_for_voice(segments, audio_data, sr)
            
            # 4. ç²¾ç¢ºé‚Šç•Œä¿®æ­£
            refined_segments = self._refine_voice_boundaries(voice_segments, audio_data, sr)
            
            # 5. å ±å‘Šæª¢æ¸¬çµæœ
            self._report_voice_detection_results(segments, refined_segments)
            
            return refined_segments
            
        except Exception as e:
            logger.error(f"äººè²æª¢æ¸¬å¤±æ•—: {e}")
            return segments
    
    def _load_audio(self, audio_file: str) -> Tuple[Optional[np.ndarray], int]:
        """è¼‰å…¥éŸ³é »æ–‡ä»¶"""
        try:
            if not Path(audio_file).exists():
                logger.error(f"éŸ³é »æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
                return None, 0
            
            # ä½¿ç”¨ librosa è¼‰å…¥éŸ³é »
            audio_data, sr = librosa.load(audio_file, sr=self.sample_rate)
            logger.info(f"éŸ³é »è¼‰å…¥æˆåŠŸ: {len(audio_data)/sr:.2f}ç§’, æ¡æ¨£ç‡={sr}Hz")
            
            return audio_data, sr
            
        except Exception as e:
            logger.error(f"éŸ³é »è¼‰å…¥å¤±æ•—: {e}")
            return None, 0
    
    def _analyze_global_features(self, audio_data: np.ndarray, sr: int) -> None:
        """åˆ†æå…¨åŸŸéŸ³é »ç‰¹å¾µä¸¦å»ºç«‹å‹•æ…‹æ¨¡å‹"""
        try:
            logger.info("åˆ†æå…¨åŸŸéŸ³é »ç‰¹å¾µ...")
            
            # æå–å…¨åŸŸç‰¹å¾µ
            global_features = self._extract_global_audio_features(audio_data, sr)
            
            # åˆ†æ®µç‰¹å¾µæå–ï¼ˆç”¨æ–¼å­¸ç¿’ï¼‰
            segment_features = []
            window_size = sr * 2  # 2ç§’çª—å£
            hop_size = sr // 4    # 0.25ç§’è·³èº
            
            for start in range(0, len(audio_data) - window_size, hop_size):
                end = start + window_size
                segment_audio = audio_data[start:end]
                
                features = self._extract_segment_features(segment_audio, sr)
                if features is not None:
                    segment_features.append(features)
            
            if not segment_features:
                logger.warning("ç„¡æ³•æå–æ®µè½ç‰¹å¾µ")
                return
            
            # è½‰æ›ç‚ºçŸ©é™£
            feature_matrix = np.array(segment_features)
            
            # æ¨™æº–åŒ–ç‰¹å¾µ
            self.scaler.fit(feature_matrix)
            normalized_features = self.scaler.transform(feature_matrix)
            
            # ç„¡ç›£ç£èšé¡å­¸ç¿’ï¼ˆå‡è¨­å­˜åœ¨äººè²å’Œéäººè²å…©é¡ï¼‰
            self._learn_voice_nonvoice_clusters(normalized_features)
            
            # å‹•æ…‹è¨­å®šæª¢æ¸¬é–¾å€¼
            self._calculate_dynamic_thresholds(feature_matrix)
            
            logger.info("å…¨åŸŸç‰¹å¾µåˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"å…¨åŸŸç‰¹å¾µåˆ†æå¤±æ•—: {e}")
    
    def _extract_global_audio_features(self, audio_data: np.ndarray, sr: int) -> Dict:
        """æå–å…¨åŸŸéŸ³é »ç‰¹å¾µ"""
        features = {}
        
        # é »è­œç‰¹å¾µ
        stft = librosa.stft(audio_data, hop_length=self.hop_length, 
                           n_fft=self.frame_length * 2)
        magnitude = np.abs(stft)
        
        # é »è­œè³ªå¿ƒ
        features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(
            S=magnitude, sr=sr)[0])
        
        # é »è­œå¸¶å¯¬
        features['spectral_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(
            S=magnitude, sr=sr)[0])
        
        # é »è­œæ»¾é™
        features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(
            S=magnitude, sr=sr)[0])
        
        # é›¶äº¤å‰ç‡
        features['zcr'] = np.mean(librosa.feature.zero_crossing_rate(
            audio_data, frame_length=self.frame_length, hop_length=self.hop_length)[0])
        
        # èƒ½é‡çµ±è¨ˆ
        features['energy_mean'] = np.mean(audio_data ** 2)
        features['energy_std'] = np.std(audio_data ** 2)
        
        return features
    
    def _extract_segment_features(self, segment_audio: np.ndarray, sr: int) -> Optional[np.ndarray]:
        """æå–éŸ³é »æ®µè½çš„å¤šç¶­ç‰¹å¾µå‘é‡"""
        try:
            if len(segment_audio) < self.frame_length:
                return None
            
            features = []
            
            # 1. åŸºé »ç‰¹å¾µï¼ˆF0ï¼‰
            try:
                f0, voiced_flag, voiced_probs = librosa.pyin(
                    segment_audio, 
                    fmin=librosa.note_to_hz('C2'),  # ~65 Hz
                    fmax=librosa.note_to_hz('C7')   # ~2093 Hz
                )
                f0_clean = f0[voiced_flag]
                if len(f0_clean) > 0:
                    features.extend([
                        np.mean(f0_clean),      # å¹³å‡åŸºé »
                        np.std(f0_clean),       # åŸºé »è®ŠåŒ–
                        np.mean(voiced_probs)   # æœ‰è²æ¦‚ç‡
                    ])
                else:
                    features.extend([0.0, 0.0, 0.0])
            except:
                features.extend([0.0, 0.0, 0.0])
            
            # 2. æ¢…çˆ¾é »ç‡å€’è­œä¿‚æ•¸ (MFCC) - èªéŸ³è­˜åˆ¥æ ¸å¿ƒç‰¹å¾µ
            mfccs = librosa.feature.mfcc(
                y=segment_audio, sr=sr, n_mfcc=13,
                hop_length=self.hop_length, n_fft=self.frame_length * 2
            )
            features.extend(np.mean(mfccs, axis=1))  # 13ç¶­MFCCå¹³å‡å€¼
            
            # 3. é »è­œç‰¹å¾µ
            stft = librosa.stft(segment_audio, hop_length=self.hop_length, 
                               n_fft=self.frame_length * 2)
            magnitude = np.abs(stft)
            
            # é »è­œè³ªå¿ƒï¼ˆéŸ³è‰²ç‰¹å¾µï¼‰
            spectral_centroid = librosa.feature.spectral_centroid(
                S=magnitude, sr=sr)
            features.append(np.mean(spectral_centroid))
            
            # é »è­œå¸¶å¯¬ï¼ˆé »è­œåˆ†æ•£åº¦ï¼‰
            spectral_bandwidth = librosa.feature.spectral_bandwidth(
                S=magnitude, sr=sr)
            features.append(np.mean(spectral_bandwidth))
            
            # 4. å…±æŒ¯å³°ç›¸é—œç‰¹å¾µï¼ˆäººè²ç‰¹æœ‰ï¼‰
            # è¨ˆç®—ç·šæ€§é æ¸¬ä¿‚æ•¸ï¼ˆLPCï¼‰è¿‘ä¼¼å…±æŒ¯å³°
            try:
                lpc_coeffs = librosa.lpc(segment_audio, order=12)
                # å¾LPCä¿‚æ•¸è¨ˆç®—å…±æŒ¯å³°é »ç‡
                roots = np.roots(lpc_coeffs)
                roots = roots[np.imag(roots) >= 0]
                
                formant_frequencies = []
                for root in roots:
                    if np.abs(root) < 1.0:
                        freq = np.angle(root) * sr / (2 * np.pi)
                        if 200 < freq < 4000:  # äººè²å…±æŒ¯å³°ç¯„åœ
                            formant_frequencies.append(freq)
                
                if formant_frequencies:
                    formant_frequencies.sort()
                    # å‰ä¸‰å€‹å…±æŒ¯å³°
                    for i in range(3):
                        if i < len(formant_frequencies):
                            features.append(formant_frequencies[i])
                        else:
                            features.append(0.0)
                else:
                    features.extend([0.0, 0.0, 0.0])
            except:
                features.extend([0.0, 0.0, 0.0])
            
            # 5. æ™‚åŸŸç‰¹å¾µ
            # é›¶äº¤å‰ç‡ï¼ˆèªéŸ³vséŸ³æ¨‚å€åˆ†ï¼‰
            zcr = librosa.feature.zero_crossing_rate(
                segment_audio, frame_length=self.frame_length, 
                hop_length=self.hop_length)
            features.append(np.mean(zcr))
            
            # çŸ­æ™‚èƒ½é‡
            energy = librosa.feature.rms(
                y=segment_audio, frame_length=self.frame_length, 
                hop_length=self.hop_length)
            features.extend([np.mean(energy), np.std(energy)])
            
            # 6. é »è­œå¹³å¦åº¦ï¼ˆéŸ³æ¨‚vsèªéŸ³æŒ‡æ¨™ï¼‰
            spectral_flatness = librosa.feature.spectral_flatness(
                S=magnitude)
            features.append(np.mean(spectral_flatness))
            
            return np.array(features)
            
        except Exception as e:
            logger.debug(f"ç‰¹å¾µæå–å¤±æ•—: {e}")
            return None
    
    def _learn_voice_nonvoice_clusters(self, feature_matrix: np.ndarray) -> None:
        """å­¸ç¿’äººè²/éäººè²èšé¡æ¨¡å‹"""
        try:
            if len(feature_matrix) < 4:
                logger.warning("æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•é€²è¡Œèšé¡å­¸ç¿’")
                return
            
            # K-meansèšé¡ï¼ˆå‡è¨­2é¡ï¼šäººè²å’Œéäººè²ï¼‰
            centroids, labels = kmeans2(feature_matrix, 2)
            
            # åˆ†æå…©å€‹èšé¡çš„ç‰¹å¾µ
            cluster_0_features = feature_matrix[labels == 0]
            cluster_1_features = feature_matrix[labels == 1]
            
            # åŸºæ–¼MFCCå’ŒåŸºé »ç‰¹å¾µåˆ¤æ–·å“ªå€‹æ˜¯äººè²èšé¡
            # äººè²é€šå¸¸å…·æœ‰ï¼šæ›´æ˜é¡¯çš„åŸºé »ã€ç‰¹å®šçš„MFCCæ¨¡å¼
            def voice_likelihood(features):
                # åŸºé »ç›¸é—œç‰¹å¾µ (indices 0-2)
                f0_mean = np.mean(features[:, 0])
                voiced_prob = np.mean(features[:, 2])
                
                # MFCCç‰¹å¾µç©©å®šæ€§
                mfcc_features = features[:, 3:16]  # MFCCä¿‚æ•¸
                mfcc_stability = 1.0 / (np.mean(np.std(mfcc_features, axis=0)) + 1e-8)
                
                # é›¶äº¤å‰ç‡ï¼ˆèªéŸ³é©ä¸­ï¼ŒéŸ³æ¨‚å¯èƒ½æ›´ä½æˆ–æ›´é«˜ï¼‰
                zcr_mean = np.mean(features[:, -4])
                zcr_score = 1.0 - abs(zcr_mean - 0.1)  # èªéŸ³ZCRé€šå¸¸around 0.1
                
                return (voiced_prob * 2.0 + mfcc_stability * 1.0 + zcr_score * 1.0) / 4.0
            
            voice_score_0 = voice_likelihood(cluster_0_features)
            voice_score_1 = voice_likelihood(cluster_1_features)
            
            if voice_score_0 > voice_score_1:
                self.voice_cluster_id = 0
                self.voice_centroid = centroids[0]
                logger.info(f"äººè²èšé¡: 0 (è©•åˆ†: {voice_score_0:.3f} vs {voice_score_1:.3f})")
            else:
                self.voice_cluster_id = 1
                self.voice_centroid = centroids[1]
                logger.info(f"äººè²èšé¡: 1 (è©•åˆ†: {voice_score_1:.3f} vs {voice_score_0:.3f})")
            
            self.cluster_centroids = centroids
            self.cluster_labels = labels
            
        except Exception as e:
            logger.error(f"èšé¡å­¸ç¿’å¤±æ•—: {e}")
            self.voice_cluster_id = None
    
    def _calculate_dynamic_thresholds(self, feature_matrix: np.ndarray) -> None:
        """å‹•æ…‹è¨ˆç®—æª¢æ¸¬é–¾å€¼"""
        try:
            # åŸºæ–¼ç‰¹å¾µåˆ†ä½ˆçš„çµ±è¨ˆæ–¹æ³•è¨­å®šé–¾å€¼
            if hasattr(self, 'voice_cluster_id') and self.voice_cluster_id is not None:
                voice_features = feature_matrix[self.cluster_labels == self.voice_cluster_id]
                
                # äººè²ç½®ä¿¡åº¦é–¾å€¼ï¼ˆåŸºæ–¼è·é›¢åˆ†ä½ˆçš„åˆ†ä½æ•¸ï¼‰
                distances = []
                for feature in feature_matrix:
                    normalized_feature = self.scaler.transform([feature])[0]
                    distance = np.linalg.norm(normalized_feature - self.voice_centroid)
                    distances.append(distance)
                
                distances = np.array(distances)
                
                # ä½¿ç”¨75%åˆ†ä½æ•¸ä½œç‚ºé–¾å€¼ï¼ˆå‹•æ…‹èª¿æ•´ï¼‰
                self.voice_confidence_threshold = np.percentile(distances, 75)
                
                # æ®µè½æœ€å°äººè²æ¯”ä¾‹ï¼ˆåŸºæ–¼èšé¡çµæœå‹•æ…‹è¨­å®šï¼‰
                voice_ratio = len(voice_features) / len(feature_matrix)
                self.segment_min_voice_ratio = max(0.3, voice_ratio * 0.6)
                
                logger.info(f"å‹•æ…‹é–¾å€¼è¨­å®šå®Œæˆ:")
                logger.info(f"  äººè²ç½®ä¿¡åº¦é–¾å€¼: {self.voice_confidence_threshold:.3f}")
                logger.info(f"  æœ€å°äººè²æ¯”ä¾‹: {self.segment_min_voice_ratio:.3f}")
            else:
                # é è¨­ä¿å®ˆå€¼
                self.voice_confidence_threshold = 1.0
                self.segment_min_voice_ratio = 0.5
                
        except Exception as e:
            logger.error(f"é–¾å€¼è¨ˆç®—å¤±æ•—: {e}")
            self.voice_confidence_threshold = 1.0
            self.segment_min_voice_ratio = 0.5
    
    def _process_segments_for_voice(self, segments: List[Dict], 
                                  audio_data: np.ndarray, sr: int) -> List[Dict]:
        """ç‚ºæ¯å€‹æ®µè½é€²è¡Œäººè²æª¢æ¸¬"""
        processed_segments = []
        
        for i, segment in enumerate(segments):
            try:
                start_time = segment.get('start', 0.0)
                end_time = segment.get('end', start_time + 1.0)
                
                # æå–æ®µè½éŸ³é »
                start_sample = int(start_time * sr)
                end_sample = int(end_time * sr)
                
                if end_sample > len(audio_data):
                    end_sample = len(audio_data)
                
                if end_sample <= start_sample:
                    processed_segments.append(segment)
                    continue
                
                segment_audio = audio_data[start_sample:end_sample]
                
                # åˆ†ææ®µè½çš„äººè²å…§å®¹
                voice_analysis = self._analyze_segment_voice_content(segment_audio, sr)
                
                # æ±ºå®šæ˜¯å¦ä¿ç•™æ­¤æ®µè½
                segment_copy = segment.copy()
                segment_copy['voice_analysis'] = voice_analysis
                
                if voice_analysis['is_voice_segment']:
                    # ä¿ç•™ç‚ºäººè²æ®µè½ï¼Œå¯èƒ½éœ€è¦èª¿æ•´é‚Šç•Œ
                    processed_segments.append(segment_copy)
                    logger.debug(f"æ®µè½ {i+1} åˆ¤å®šç‚ºäººè²: {voice_analysis['voice_confidence']:.3f}")
                else:
                    # æ¨™è¨˜ç‚ºéäººè²æ®µè½
                    segment_copy['filtered_reason'] = 'non_voice_content'
                    logger.info(f"æ®µè½ {i+1} éæ¿¾ï¼ˆéäººè²ï¼‰: ã€Œ{segment.get('text', '')}ã€"
                              f" (ç½®ä¿¡åº¦: {voice_analysis['voice_confidence']:.3f})")
                
            except Exception as e:
                logger.error(f"æ®µè½ {i+1} è™•ç†å¤±æ•—: {e}")
                processed_segments.append(segment)
        
        return processed_segments
    
    def _analyze_segment_voice_content(self, segment_audio: np.ndarray, sr: int) -> Dict:
        """åˆ†ææ®µè½çš„äººè²å…§å®¹"""
        try:
            # æ»‘å‹•çª—å£åˆ†æ
            window_size = sr // 2  # 0.5ç§’çª—å£
            hop_size = sr // 4     # 0.25ç§’è·³èº
            
            voice_scores = []
            
            for start in range(0, len(segment_audio) - window_size, hop_size):
                end = start + window_size
                window_audio = segment_audio[start:end]
                
                features = self._extract_segment_features(window_audio, sr)
                if features is not None:
                    # è¨ˆç®—èˆ‡äººè²èšé¡ä¸­å¿ƒçš„è·é›¢
                    if hasattr(self, 'voice_centroid'):
                        normalized_features = self.scaler.transform([features])[0]
                        distance = np.linalg.norm(normalized_features - self.voice_centroid)
                        voice_score = 1.0 / (1.0 + distance)  # è·é›¢è¶Šå°ï¼Œè©•åˆ†è¶Šé«˜
                        voice_scores.append(voice_score)
            
            if not voice_scores:
                return {
                    'is_voice_segment': False,
                    'voice_confidence': 0.0,
                    'voice_ratio': 0.0
                }
            
            # è¨ˆç®—æ®µè½çš„äººè²æŒ‡æ¨™
            voice_scores = np.array(voice_scores)
            mean_voice_score = np.mean(voice_scores)
            
            # åŸºæ–¼å‹•æ…‹é–¾å€¼åˆ¤å®š
            is_voice = (mean_voice_score > 1.0 / (1.0 + self.voice_confidence_threshold))
            voice_ratio = np.sum(voice_scores > 0.5) / len(voice_scores)
            
            return {
                'is_voice_segment': is_voice and voice_ratio > self.segment_min_voice_ratio,
                'voice_confidence': mean_voice_score,
                'voice_ratio': voice_ratio,
                'voice_scores': voice_scores.tolist()
            }
            
        except Exception as e:
            logger.error(f"äººè²å…§å®¹åˆ†æå¤±æ•—: {e}")
            return {
                'is_voice_segment': True,  # é è¨­ä¿ç•™
                'voice_confidence': 0.5,
                'voice_ratio': 0.5
            }
    
    def _refine_voice_boundaries(self, segments: List[Dict], 
                               audio_data: np.ndarray, sr: int) -> List[Dict]:
        """ç²¾ç¢ºä¿®æ­£äººè²é‚Šç•Œ"""
        refined_segments = []
        
        for segment in segments:
            # è·³éè¢«éæ¿¾çš„æ®µè½
            if 'filtered_reason' in segment:
                continue
            
            try:
                start_time = segment.get('start', 0.0)
                end_time = segment.get('end', start_time + 1.0)
                
                # ç²¾ç¢ºé‚Šç•Œæª¢æ¸¬
                refined_start, refined_end = self._find_precise_voice_boundaries(
                    audio_data, sr, start_time, end_time
                )
                
                # å¦‚æœé‚Šç•Œè®ŠåŒ–é¡¯è‘—ï¼Œè¨˜éŒ„ä¿®æ­£
                if abs(refined_start - start_time) > 0.1 or abs(refined_end - end_time) > 0.1:
                    logger.info(f"é‚Šç•Œä¿®æ­£: {start_time:.3f}s-{end_time:.3f}s â†’ "
                              f"{refined_start:.3f}s-{refined_end:.3f}s "
                              f"ã€Œ{segment.get('text', '')}ã€")
                    
                    segment_copy = segment.copy()
                    segment_copy['start'] = refined_start
                    segment_copy['end'] = refined_end
                    segment_copy['_original_start'] = start_time
                    segment_copy['_original_end'] = end_time
                    segment_copy['_boundary_refined'] = True
                    
                    refined_segments.append(segment_copy)
                else:
                    refined_segments.append(segment)
                    
            except Exception as e:
                logger.error(f"é‚Šç•Œä¿®æ­£å¤±æ•—: {e}")
                refined_segments.append(segment)
        
        return refined_segments
    
    def _find_precise_voice_boundaries(self, audio_data: np.ndarray, sr: int,
                                     start_time: float, end_time: float) -> Tuple[float, float]:
        """æ‰¾åˆ°ç²¾ç¢ºçš„äººè²é‚Šç•Œ"""
        try:
            # æ“´å±•æœç´¢ç¯„åœ
            search_margin = 2.0  # å‰å¾Œå„æ“´å±•2ç§’
            search_start_time = max(0, start_time - search_margin)
            search_end_time = min(len(audio_data) / sr, end_time + search_margin)
            
            search_start_sample = int(search_start_time * sr)
            search_end_sample = int(search_end_time * sr)
            
            search_audio = audio_data[search_start_sample:search_end_sample]
            
            # é«˜è§£æåº¦æ»‘å‹•çª—å£åˆ†æ
            window_size = sr // 10  # 0.1ç§’çª—å£
            hop_size = sr // 20     # 0.05ç§’è·³èº
            
            voice_timeline = []
            time_points = []
            
            for start in range(0, len(search_audio) - window_size, hop_size):
                end = start + window_size
                window_audio = search_audio[start:end]
                window_time = search_start_time + start / sr
                
                features = self._extract_segment_features(window_audio, sr)
                if features is not None and hasattr(self, 'voice_centroid'):
                    normalized_features = self.scaler.transform([features])[0]
                    distance = np.linalg.norm(normalized_features - self.voice_centroid)
                    voice_score = 1.0 / (1.0 + distance)
                    
                    voice_timeline.append(voice_score)
                    time_points.append(window_time)
            
            if not voice_timeline:
                return start_time, end_time
            
            voice_timeline = np.array(voice_timeline)
            time_points = np.array(time_points)
            
            # å‹•æ…‹é–¾å€¼ï¼ˆåŸºæ–¼æ•´é«”è©•åˆ†åˆ†ä½ˆï¼‰
            voice_threshold = np.percentile(voice_timeline, 60)  # 60%åˆ†ä½é»
            
            # æ‰¾åˆ°äººè²å€é–“
            voice_mask = voice_timeline > voice_threshold
            
            if not np.any(voice_mask):
                return start_time, end_time
            
            # æ‰¾åˆ°æœ€é•·çš„é€£çºŒäººè²å€é–“
            voice_regions = []
            in_voice = False
            region_start = None
            
            for i, is_voice in enumerate(voice_mask):
                if is_voice and not in_voice:
                    region_start = time_points[i]
                    in_voice = True
                elif not is_voice and in_voice:
                    if region_start is not None:
                        voice_regions.append((region_start, time_points[i-1]))
                    in_voice = False
            
            # è™•ç†æœ€å¾Œä¸€å€‹å€é–“
            if in_voice and region_start is not None:
                voice_regions.append((region_start, time_points[-1]))
            
            if not voice_regions:
                return start_time, end_time
            
            # é¸æ“‡èˆ‡åŸå§‹æ®µè½é‡ç–Šæœ€å¤šçš„å€é–“
            best_region = None
            max_overlap = 0
            
            for region_start, region_end in voice_regions:
                overlap = min(end_time, region_end) - max(start_time, region_start)
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_region = (region_start, region_end)
            
            if best_region and max_overlap > 0:
                return best_region[0], best_region[1]
            else:
                return start_time, end_time
                
        except Exception as e:
            logger.error(f"ç²¾ç¢ºé‚Šç•Œæª¢æ¸¬å¤±æ•—: {e}")
            return start_time, end_time
    
    def _report_voice_detection_results(self, original: List[Dict], processed: List[Dict]) -> None:
        """å ±å‘Šäººè²æª¢æ¸¬çµæœ"""
        filtered_count = 0
        refined_count = 0
        
        # çµ±è¨ˆè¢«éæ¿¾çš„æ®µè½
        original_count = len(original)
        processed_count = len([s for s in processed if 'filtered_reason' not in s])
        filtered_count = original_count - processed_count
        
        # çµ±è¨ˆé‚Šç•Œä¿®æ­£çš„æ®µè½
        refined_count = len([s for s in processed if s.get('_boundary_refined', False)])
        
        logger.info("ğŸ¯ è‡ªé©æ‡‰äººè²æª¢æ¸¬çµæœ:")
        logger.info(f"  åŸå§‹æ®µè½: {original_count}")
        logger.info(f"  ä¿ç•™æ®µè½: {processed_count}")
        logger.info(f"  éæ¿¾æ®µè½: {filtered_count} (éäººè²å…§å®¹)")
        logger.info(f"  é‚Šç•Œä¿®æ­£: {refined_count}")
        
        if filtered_count > 0:
            logger.info("âœ… æˆåŠŸéæ¿¾éäººè²æ®µè½ï¼Œåƒ…ä¿ç•™çœŸå¯¦èªéŸ³å…§å®¹")


def apply_adaptive_voice_detection(segments: List[Dict], audio_file: str) -> List[Dict]:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šæ‡‰ç”¨è‡ªé©æ‡‰äººè²æª¢æ¸¬
    
    Args:
        segments: å­—å¹•æ®µè½åˆ—è¡¨
        audio_file: éŸ³é »æ–‡ä»¶è·¯å¾‘
        
    Returns:
        List[Dict]: æª¢æ¸¬å¾Œçš„æ®µè½ï¼ˆåƒ…åŒ…å«äººè²éƒ¨åˆ†ï¼‰
    """
    try:
        detector = AdaptiveVoiceDetector()
        return detector.detect_voice_segments(segments, audio_file)
    except Exception as e:
        logger.warning(f"è‡ªé©æ‡‰äººè²æª¢æ¸¬è™•ç†ç•°å¸¸: {e}")
        return segments