#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‰µå»ºç¶œåˆæ¸¬è©¦æ•¸æ“šé›†
ç‚ºæ€§èƒ½ç›£æ§ç³»çµ±å»ºç«‹å¤šæ¨£åŒ–çš„éŸ³é »æ¸¬è©¦é›†åˆï¼Œæ¶µè“‹ä¸åŒæ™‚é•·ã€å…§å®¹é¡å‹å’Œå“è³ª
"""

import os
import sys
import json
import time
import shutil
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# è¨­å®šæ§åˆ¶å°ç·¨ç¢¼
if sys.platform.startswith('win'):
    import os
    os.system('chcp 65001 > NUL')
    sys.stdout.reconfigure(encoding='utf-8')

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('test_dataset_creation.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class ComprehensiveTestDatasetManager:
    """ç¶œåˆæ¸¬è©¦æ•¸æ“šé›†ç®¡ç†å™¨"""
    
    def __init__(self, base_dir: str = None):
        if base_dir is None:
            self.base_dir = Path(__file__).parent / "test_datasets"
        else:
            self.base_dir = Path(base_dir)
        
        # ç¢ºä¿åŸºç¤ç›®éŒ„å­˜åœ¨
        self.base_dir.mkdir(exist_ok=True)
        
        # æ¸¬è©¦åˆ†é¡ç›®éŒ„
        self.categories = {
            'short': self.base_dir / "short_clips",      # çŸ­ç‰‡æ®µ (5-30ç§’)
            'medium': self.base_dir / "medium_clips",    # ä¸­ç­‰é•·åº¦ (30ç§’-5åˆ†é˜)
            'long': self.base_dir / "long_clips",        # é•·éŸ³é » (5-30åˆ†é˜)
            'very_long': self.base_dir / "very_long_clips", # è¶…é•·éŸ³é » (30åˆ†é˜+)
            'multilingual': self.base_dir / "multilingual", # å¤šèªè¨€æ¸¬è©¦
            'quality_tests': self.base_dir / "quality_tests", # å“è³ªæ¸¬è©¦
            'special_cases': self.base_dir / "special_cases"  # ç‰¹æ®Šæƒ…æ³
        }
        
        # å‰µå»ºåˆ†é¡ç›®éŒ„
        for category_path in self.categories.values():
            category_path.mkdir(exist_ok=True)
    
    def get_existing_test_files(self) -> Dict[str, List[str]]:
        """æœå°‹ç¾æœ‰çš„æ¸¬è©¦éŸ³é »æª”æ¡ˆ"""
        logger.info("=== æƒæç¾æœ‰æ¸¬è©¦æª”æ¡ˆ ===")
        
        # æœå°‹è·¯å¾‘
        search_paths = [
            Path(__file__).parent.parent / "test_VIDEO",  # ä¸»è¦æ¸¬è©¦ç›®éŒ„
            Path("C:/Users/USER-ART0/Desktop"),           # æ¡Œé¢
            Path(__file__).parent,                        # ç•¶å‰ç›®éŒ„
        ]
        
        existing_files = {}
        audio_extensions = {'.mp3', '.wav', '.mp4', '.m4a', '.flac', '.ogg', '.aac'}
        
        for search_path in search_paths:
            if search_path.exists():
                logger.info(f"æœå°‹è·¯å¾‘: {search_path}")
                files = []
                
                for file_path in search_path.rglob('*'):
                    if file_path.is_file() and file_path.suffix.lower() in audio_extensions:
                        # ä¼°ç®—æª”æ¡ˆå¤§å°å’Œå¯èƒ½çš„æ™‚é•·
                        file_size = file_path.stat().st_size
                        estimated_duration = self._estimate_audio_duration(file_path)
                        
                        files.append({
                            'path': str(file_path),
                            'name': file_path.name,
                            'size': file_size,
                            'size_mb': round(file_size / (1024*1024), 2),
                            'estimated_duration': estimated_duration,
                            'extension': file_path.suffix.lower()
                        })
                
                if files:
                    existing_files[str(search_path)] = files
                    logger.info(f"  æ‰¾åˆ° {len(files)} å€‹éŸ³é »æª”æ¡ˆ")
        
        return existing_files
    
    def _estimate_audio_duration(self, file_path: Path) -> float:
        """ä¼°ç®—éŸ³é »æ™‚é•·ï¼ˆåŸºæ–¼æª”æ¡ˆå¤§å°å’Œé¡å‹ï¼‰"""
        file_size = file_path.stat().st_size / (1024*1024)  # MB
        extension = file_path.suffix.lower()
        filename = file_path.name.lower()
        
        # åŸºæ–¼æª”æ¡ˆåç¨±çš„å•Ÿç™¼å¼ä¼°ç®—
        if any(keyword in filename for keyword in ['short', 'test', 'sample']):
            return min(30, file_size * 8)  # æ¸¬è©¦æª”æ¡ˆé€šå¸¸è¼ƒçŸ­
        elif any(keyword in filename for keyword in ['song', 'music', 'éŸ³æ¨‚']):
            return min(300, file_size * 6)  # éŸ³æ¨‚æª”æ¡ˆ
        elif 'interview' in filename or 'è¨ªè«‡' in filename:
            return min(1800, file_size * 4)  # è¨ªè«‡å¯èƒ½è¼ƒé•·
        
        # åŸºæ–¼æª”æ¡ˆå¤§å°çš„é€šç”¨ä¼°ç®—
        if extension == '.mp3':
            return file_size * 8  # ç´„1MB/min for 128kbps
        elif extension == '.wav':
            return file_size / 10  # ç´„10MB/min for CD quality
        elif extension == '.mp4':
            return file_size * 2  # è¦–é »æª”æ¡ˆéŸ³è»Œä¼°ç®—
        else:
            return file_size * 6  # ä¸€èˆ¬å£“ç¸®éŸ³é »
    
    def categorize_existing_files(self, existing_files: Dict[str, List[str]]) -> Dict[str, List[Dict]]:
        """å°‡ç¾æœ‰æª”æ¡ˆæŒ‰ç…§æ¸¬è©¦éœ€æ±‚åˆ†é¡"""
        logger.info("=== æª”æ¡ˆåˆ†é¡è™•ç† ===")
        
        categorized = {
            'short': [],      # 5-30ç§’
            'medium': [],     # 30ç§’-5åˆ†é˜
            'long': [],       # 5-30åˆ†é˜
            'very_long': [],  # 30åˆ†é˜+
            'multilingual': [],
            'quality_tests': [],
            'special_cases': []
        }
        
        for search_path, files in existing_files.items():
            for file_info in files:
                duration = file_info['estimated_duration']
                filename = file_info['name'].lower()
                
                # æ™‚é•·åˆ†é¡
                if duration <= 30:
                    categorized['short'].append(file_info)
                elif duration <= 300:  # 5åˆ†é˜
                    categorized['medium'].append(file_info)
                elif duration <= 1800:  # 30åˆ†é˜
                    categorized['long'].append(file_info)
                else:
                    categorized['very_long'].append(file_info)
                
                # å¤šèªè¨€æª¢æ¸¬
                if any(lang in filename for lang in ['en', 'english', 'jp', 'japanese', 'ko', 'korean']):
                    categorized['multilingual'].append(file_info)
                
                # å“è³ªæ¸¬è©¦æª¢æ¸¬
                if any(quality in filename for quality in ['hd', 'high', 'é«˜å“è³ª', 'lossless']):
                    categorized['quality_tests'].append(file_info)
                elif any(quality in filename for quality in ['low', 'ä½å“è³ª', 'compressed']):
                    categorized['quality_tests'].append(file_info)
                
                # ç‰¹æ®Šæƒ…æ³æª¢æ¸¬
                if any(special in filename for special in ['noise', 'é›œéŸ³', 'echo', 'å›éŸ³', 'fast', 'å¿«é€Ÿ', 'slow', 'æ…¢é€Ÿ']):
                    categorized['special_cases'].append(file_info)
        
        # è¼¸å‡ºåˆ†é¡çµæœ
        for category, files in categorized.items():
            logger.info(f"{category}: {len(files)} å€‹æª”æ¡ˆ")
            for file_info in files[:3]:  # é¡¯ç¤ºå‰3å€‹ä½œç‚ºç¯„ä¾‹
                logger.info(f"  - {file_info['name']} ({file_info['estimated_duration']:.1f}s, {file_info['size_mb']}MB)")
        
        return categorized
    
    def create_test_dataset_manifest(self, categorized_files: Dict[str, List[Dict]]) -> Dict:
        """å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†æ¸…å–®"""
        logger.info("=== å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†æ¸…å–® ===")
        
        manifest = {
            'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_files': sum(len(files) for files in categorized_files.values()),
            'categories': {},
            'performance_test_plan': {},
            'recommended_test_sequence': []
        }
        
        # è™•ç†å„åˆ†é¡
        for category, files in categorized_files.items():
            if not files:
                continue
                
            manifest['categories'][category] = {
                'count': len(files),
                'files': files,
                'total_duration': sum(f['estimated_duration'] for f in files),
                'total_size_mb': sum(f['size_mb'] for f in files),
                'recommended_for_performance_testing': self._get_performance_test_recommendations(category, files)
            }
        
        # æ€§èƒ½æ¸¬è©¦è¨ˆåŠƒ
        manifest['performance_test_plan'] = self._create_performance_test_plan(categorized_files)
        
        # æ¨è–¦æ¸¬è©¦é †åº
        manifest['recommended_test_sequence'] = self._create_test_sequence(categorized_files)
        
        return manifest
    
    def _get_performance_test_recommendations(self, category: str, files: List[Dict]) -> Dict:
        """ç²å–æ€§èƒ½æ¸¬è©¦å»ºè­°"""
        if category == 'short':
            return {
                'priority': 'high',
                'test_focus': 'latency_optimization',
                'expected_rtf_range': '0.05-0.2',
                'performance_modes': ['auto', 'gpu', 'cpu'],
                'notes': 'çŸ­ç‰‡æ®µæ¸¬è©¦å•Ÿå‹•å»¶é²å’Œæœ€å°è™•ç†å–®å…ƒæ•ˆèƒ½'
            }
        elif category == 'medium':
            return {
                'priority': 'high',
                'test_focus': 'standard_performance',
                'expected_rtf_range': '0.1-0.5',
                'performance_modes': ['auto', 'gpu'],
                'notes': 'æ¨™æº–æ€§èƒ½åŸºæº–æ¸¬è©¦ï¼Œæœ€å¸¸è¦‹çš„ä½¿ç”¨å ´æ™¯'
            }
        elif category == 'long':
            return {
                'priority': 'medium',
                'test_focus': 'memory_management',
                'expected_rtf_range': '0.2-0.8',
                'performance_modes': ['gpu', 'cpu'],
                'notes': 'è¨˜æ†¶é«”ç®¡ç†å’Œé•·æ™‚é–“è™•ç†ç©©å®šæ€§'
            }
        elif category == 'very_long':
            return {
                'priority': 'low',
                'test_focus': 'stress_testing',
                'expected_rtf_range': '0.5-1.5',
                'performance_modes': ['cpu'],
                'notes': 'å£“åŠ›æ¸¬è©¦ï¼Œé©—è­‰ç³»çµ±æ¥µé™å’Œç©©å®šæ€§'
            }
        else:
            return {
                'priority': 'medium',
                'test_focus': 'specialized_scenarios',
                'expected_rtf_range': '0.1-1.0',
                'performance_modes': ['auto'],
                'notes': f'{category} å°ˆç”¨æ¸¬è©¦å ´æ™¯'
            }
    
    def _create_performance_test_plan(self, categorized_files: Dict[str, List[Dict]]) -> Dict:
        """å‰µå»ºç¶œåˆæ€§èƒ½æ¸¬è©¦è¨ˆåŠƒ"""
        plan = {
            'quick_validation': {
                'description': 'å¿«é€Ÿé©—è­‰æ‰€æœ‰æ€§èƒ½æ¨¡å¼',
                'duration': '5-10åˆ†é˜',
                'files': []
            },
            'standard_benchmark': {
                'description': 'æ¨™æº–æ€§èƒ½åŸºæº–æ¸¬è©¦',
                'duration': '15-30åˆ†é˜',
                'files': []
            },
            'comprehensive_test': {
                'description': 'å…¨é¢æ€§èƒ½å’Œç©©å®šæ€§æ¸¬è©¦',
                'duration': '1-2å°æ™‚',
                'files': []
            },
            'stress_test': {
                'description': 'å£“åŠ›æ¸¬è©¦å’Œæ¥µé™æƒ…æ³',
                'duration': '2-4å°æ™‚',
                'files': []
            }
        }
        
        # å¿«é€Ÿé©—è­‰ - é¸æ“‡æœ€çŸ­çš„æª”æ¡ˆ
        if categorized_files.get('short'):
            plan['quick_validation']['files'].extend(
                sorted(categorized_files['short'], key=lambda x: x['estimated_duration'])[:2]
            )
        
        # æ¨™æº–åŸºæº– - ä¸­ç­‰é•·åº¦æª”æ¡ˆ
        if categorized_files.get('medium'):
            plan['standard_benchmark']['files'].extend(
                sorted(categorized_files['medium'], key=lambda x: x['estimated_duration'])[:3]
            )
        
        # å…¨é¢æ¸¬è©¦ - å„é¡å‹æª”æ¡ˆ
        for category in ['short', 'medium', 'long']:
            if categorized_files.get(category):
                plan['comprehensive_test']['files'].extend(
                    sorted(categorized_files[category], key=lambda x: x['estimated_duration'])[:2]
                )
        
        # å£“åŠ›æ¸¬è©¦ - é•·éŸ³é »æª”æ¡ˆ
        if categorized_files.get('long'):
            plan['stress_test']['files'].extend(categorized_files['long'])
        if categorized_files.get('very_long'):
            plan['stress_test']['files'].extend(categorized_files['very_long'])
        
        return plan
    
    def _create_test_sequence(self, categorized_files: Dict[str, List[Dict]]) -> List[Dict]:
        """å‰µå»ºæ¨è–¦çš„æ¸¬è©¦åŸ·è¡Œé †åº"""
        sequence = []
        
        # 1. å¿«é€Ÿç…™éœ§æ¸¬è©¦
        sequence.append({
            'phase': 'smoke_test',
            'description': 'å¿«é€Ÿç…™éœ§æ¸¬è©¦ - é©—è­‰åŸºæœ¬åŠŸèƒ½',
            'files': sorted(categorized_files.get('short', []), key=lambda x: x['estimated_duration'])[:1],
            'performance_modes': ['auto'],
            'expected_duration': '1-2åˆ†é˜'
        })
        
        # 2. æ€§èƒ½æ¨¡å¼é©—è­‰
        sequence.append({
            'phase': 'performance_modes',
            'description': 'æ€§èƒ½æ¨¡å¼é©—è­‰ - æ¸¬è©¦æ‰€æœ‰æ¨¡å¼',
            'files': sorted(categorized_files.get('medium', []), key=lambda x: x['estimated_duration'])[:1],
            'performance_modes': ['auto', 'gpu', 'cpu'],
            'expected_duration': '5-10åˆ†é˜'
        })
        
        # 3. æ™‚é•·ç¯„åœæ¸¬è©¦
        sequence.append({
            'phase': 'duration_range',
            'description': 'æ™‚é•·ç¯„åœæ¸¬è©¦ - ä¸åŒæ™‚é•·æª”æ¡ˆ',
            'files': self._select_representative_files(categorized_files),
            'performance_modes': ['auto'],
            'expected_duration': '10-20åˆ†é˜'
        })
        
        # 4. å“è³ªå’Œç‰¹æ®Šæƒ…æ³
        sequence.append({
            'phase': 'quality_special',
            'description': 'å“è³ªå’Œç‰¹æ®Šæƒ…æ³æ¸¬è©¦',
            'files': categorized_files.get('quality_tests', [])[:2] + categorized_files.get('special_cases', [])[:2],
            'performance_modes': ['auto'],
            'expected_duration': '5-15åˆ†é˜'
        })
        
        # 5. å¤šèªè¨€æ¸¬è©¦
        if categorized_files.get('multilingual'):
            sequence.append({
                'phase': 'multilingual',
                'description': 'å¤šèªè¨€æ”¯æ´æ¸¬è©¦',
                'files': categorized_files['multilingual'][:3],
                'performance_modes': ['auto'],
                'expected_duration': '10-30åˆ†é˜'
            })
        
        return sequence
    
    def _select_representative_files(self, categorized_files: Dict[str, List[Dict]]) -> List[Dict]:
        """é¸æ“‡ä»£è¡¨æ€§æª”æ¡ˆé€²è¡Œæ™‚é•·ç¯„åœæ¸¬è©¦"""
        representative = []
        
        # æ¯å€‹æ™‚é•·åˆ†é¡é¸ä¸€å€‹ä»£è¡¨æª”æ¡ˆ
        for category in ['short', 'medium', 'long']:
            files = categorized_files.get(category, [])
            if files:
                # é¸æ“‡ä¸­ä½æ•¸æ™‚é•·çš„æª”æ¡ˆ
                sorted_files = sorted(files, key=lambda x: x['estimated_duration'])
                mid_index = len(sorted_files) // 2
                representative.append(sorted_files[mid_index])
        
        return representative
    
    def generate_performance_test_scripts(self, manifest: Dict) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½æ¸¬è©¦è…³æœ¬"""
        logger.info("=== ç”Ÿæˆæ€§èƒ½æ¸¬è©¦è…³æœ¬ ===")
        
        script_files = []
        
        # ç‚ºæ¯å€‹æ¸¬è©¦è¨ˆåŠƒç”Ÿæˆè…³æœ¬
        for plan_name, plan_config in manifest['performance_test_plan'].items():
            script_content = self._generate_test_script(plan_name, plan_config)
            script_file = self.base_dir / f"run_{plan_name}_test.py"
            
            with open(script_file, 'w', encoding='utf-8') as f:
                f.write(script_content)
            
            script_files.append(str(script_file))
            logger.info(f"ç”Ÿæˆæ¸¬è©¦è…³æœ¬: {script_file.name}")
        
        return script_files
    
    def _generate_test_script(self, plan_name: str, plan_config: Dict) -> str:
        """ç”Ÿæˆå–®å€‹æ¸¬è©¦è¨ˆåŠƒçš„è…³æœ¬"""
        # è½‰æ›æª”æ¡ˆæ ¼å¼ä»¥ä¾¿åœ¨è…³æœ¬ä¸­ä½¿ç”¨
        test_files_str = json.dumps(plan_config['files'], ensure_ascii=False, indent=4)
        
        script_content = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªå‹•ç”Ÿæˆçš„æ€§èƒ½æ¸¬è©¦è…³æœ¬: {plan_name}
{plan_config['description']}
é ä¼°åŸ·è¡Œæ™‚é–“: {plan_config['duration']}
"""

import sys
import os
import time
import subprocess
import json
from pathlib import Path

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘ä»¥ä¾¿å°å…¥
sys.path.append(str(Path(__file__).parent.parent))

def run_performance_test():
    """åŸ·è¡Œæ€§èƒ½æ¸¬è©¦"""
    print("=== {plan_name} æ€§èƒ½æ¸¬è©¦ ===")
    print("æè¿°: {plan_config['description']}")
    print("é ä¼°æ™‚é–“: {plan_config['duration']}")
    print()
    
    test_files = {test_files_str}
    performance_modes = ['auto', 'gpu', 'cpu']
    
    results = []
    
    for file_info in test_files:
        file_path = file_info['path']
        if not Path(file_path).exists():
            print(f"è­¦å‘Š: æª”æ¡ˆä¸å­˜åœ¨ {{file_path}}")
            continue
            
        print(f"æ¸¬è©¦æª”æ¡ˆ: {{file_info['name']}} ({{file_info['estimated_duration']:.1f}}s)")
        
        for mode in performance_modes:
            print(f"  æ¸¬è©¦æ¨¡å¼: {{mode}}")
            
            # æ§‹å»ºæ¸¬è©¦å‘½ä»¤
            cmd = [
                "python",
                str(Path(__file__).parent.parent / "electron_backend.py"),
                "--files", f'["{{file_path}}"]',
                "--settings", f'{{"model":"large","language":"auto","performanceMode":"{{mode}}","outputFormat":"srt","customDir":"C:/temp/test_results","enable_gpu":{{"true" if mode=="gpu" else "false"}}}}',
                "--corrections", "[]"
            ]
            
            # åŸ·è¡Œæ¸¬è©¦
            start_time = time.time()
            try:
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
                end_time = time.time()
                
                processing_time = end_time - start_time
                rtf = processing_time / file_info['estimated_duration']
                
                test_result = {{
                    'file': file_info['name'],
                    'mode': mode,
                    'processing_time': processing_time,
                    'audio_duration': file_info['estimated_duration'],
                    'rtf': rtf,
                    'success': result.returncode == 0,
                    'stdout': result.stdout[:500],  # å‰500å­—ç¬¦
                    'stderr': result.stderr[:500] if result.stderr else None
                }}
                
                results.append(test_result)
                
                print(f"    è™•ç†æ™‚é–“: {{processing_time:.1f}}s")
                print(f"    RTF: {{rtf:.3f}}")
                print(f"    ç‹€æ…‹: {{'æˆåŠŸ' if test_result['success'] else 'å¤±æ•—'}}")
                
                if not test_result['success']:
                    print(f"    éŒ¯èª¤: {{result.stderr[:200]}}")
                
            except subprocess.TimeoutExpired:
                print(f"    è¶…æ™‚ (>10åˆ†é˜)")
                results.append({{'file': file_info['name'], 'mode': mode, 'status': 'timeout'}})
            except Exception as e:
                print(f"    ç•°å¸¸: {{e}}")
                results.append({{'file': file_info['name'], 'mode': mode, 'status': 'error', 'error': str(e)}})
            
            print()
    
    # ä¿å­˜çµæœ
    result_file = Path(__file__).parent / "{plan_name}_results.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump({{'test_name': '{plan_name}', 'results': results, 'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')}}, f, ensure_ascii=False, indent=2)
    
    print(f"æ¸¬è©¦çµæœå·²ä¿å­˜è‡³: {{result_file}}")
    return results

if __name__ == "__main__":
    results = run_performance_test()
    print(f"\\n{plan_name} æ¸¬è©¦å®Œæˆï¼Œå…±åŸ·è¡Œ {{len(results)}} å€‹æ¸¬è©¦æ¡ˆä¾‹")
'''
        
        return script_content
    
    def save_manifest(self, manifest: Dict) -> str:
        """ä¿å­˜æ¸¬è©¦æ•¸æ“šé›†æ¸…å–®"""
        manifest_file = self.base_dir / "test_dataset_manifest.json"
        
        with open(manifest_file, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ¸¬è©¦æ•¸æ“šé›†æ¸…å–®å·²ä¿å­˜è‡³: {manifest_file}")
        return str(manifest_file)
    
    def create_readme(self, manifest: Dict) -> str:
        """å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†èªªæ˜æ–‡æª”"""
        readme_content = f"""# ç¶œåˆæ¸¬è©¦æ•¸æ“šé›†

å»ºç«‹æ™‚é–“: {manifest['created_at']}
ç¸½æª”æ¡ˆæ•¸: {manifest['total_files']} å€‹

## æª”æ¡ˆåˆ†é¡

"""
        
        for category, info in manifest['categories'].items():
            readme_content += f"### {category.upper()} ({info['count']} å€‹æª”æ¡ˆ)\n"
            readme_content += f"- ç¸½æ™‚é•·: {info['total_duration']:.1f}ç§’ ({info['total_duration']/60:.1f}åˆ†é˜)\n"
            readme_content += f"- ç¸½å¤§å°: {info['total_size_mb']:.1f}MB\n"
            
            if info.get('recommended_for_performance_testing'):
                rec = info['recommended_for_performance_testing']
                readme_content += f"- æ¸¬è©¦é‡é»: {rec['test_focus']}\n"
                readme_content += f"- é æœŸRTFç¯„åœ: {rec['expected_rtf_range']}\n"
                readme_content += f"- å»ºè­°æ¨¡å¼: {', '.join(rec['performance_modes'])}\n"
            
            readme_content += "\nä»£è¡¨æ€§æª”æ¡ˆ:\n"
            for file_info in info['files'][:3]:
                readme_content += f"- {file_info['name']} ({file_info['estimated_duration']:.1f}s, {file_info['size_mb']}MB)\n"
            readme_content += "\n"
        
        readme_content += "## æ¸¬è©¦è¨ˆåŠƒ\n\n"
        for plan_name, plan_config in manifest['performance_test_plan'].items():
            readme_content += f"### {plan_name.upper()}\n"
            readme_content += f"- {plan_config['description']}\n"
            readme_content += f"- é ä¼°æ™‚é–“: {plan_config['duration']}\n"
            readme_content += f"- æ¸¬è©¦æª”æ¡ˆ: {len(plan_config['files'])} å€‹\n\n"
        
        readme_content += "## æ¨è–¦æ¸¬è©¦é †åº\n\n"
        for i, phase in enumerate(manifest['recommended_test_sequence'], 1):
            readme_content += f"{i}. **{phase['phase'].upper()}** - {phase['description']}\n"
            readme_content += f"   - é ä¼°æ™‚é–“: {phase['expected_duration']}\n"
            readme_content += f"   - æ¸¬è©¦æª”æ¡ˆ: {len(phase['files'])} å€‹\n"
            readme_content += f"   - æ€§èƒ½æ¨¡å¼: {', '.join(phase['performance_modes'])}\n\n"
        
        readme_content += """## ä½¿ç”¨æ–¹æ³•

### åŸ·è¡Œå¿«é€Ÿé©—è­‰æ¸¬è©¦
```bash
python run_quick_validation_test.py
```

### åŸ·è¡Œæ¨™æº–åŸºæº–æ¸¬è©¦
```bash
python run_standard_benchmark_test.py
```

### åŸ·è¡Œå…¨é¢æ€§èƒ½æ¸¬è©¦
```bash
python run_comprehensive_test_test.py
```

### åŸ·è¡Œå£“åŠ›æ¸¬è©¦
```bash
python run_stress_test_test.py
```

## çµæœåˆ†æ

æ¸¬è©¦çµæœå°‡ä¿å­˜ç‚ºJSONæ ¼å¼ï¼ŒåŒ…å«ï¼š
- è™•ç†æ™‚é–“å’ŒRTFå€¼
- æ€§èƒ½ç­‰ç´šåˆ†é¡
- æˆåŠŸ/å¤±æ•—ç‹€æ…‹
- è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰

çµæœæª”æ¡ˆä½ç½®ï¼š
- quick_validation_results.json
- standard_benchmark_results.json
- comprehensive_test_results.json
- stress_test_results.json
"""
        
        readme_file = self.base_dir / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info(f"æ¸¬è©¦èªªæ˜æ–‡æª”å·²å‰µå»º: {readme_file}")
        return str(readme_file)

def main():
    """ä¸»å‡½æ•¸ - å»ºç«‹ç¶œåˆæ¸¬è©¦æ•¸æ“šé›†"""
    logger.info("é–‹å§‹å»ºç«‹ç¶œåˆæ¸¬è©¦æ•¸æ“šé›†...")
    logger.info("=" * 60)
    
    try:
        # 1. åˆå§‹åŒ–ç®¡ç†å™¨
        dataset_manager = ComprehensiveTestDatasetManager()
        
        # 2. æƒæç¾æœ‰æª”æ¡ˆ
        existing_files = dataset_manager.get_existing_test_files()
        
        if not existing_files:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•æ¸¬è©¦éŸ³é »æª”æ¡ˆï¼")
            logger.info("è«‹å°‡æ¸¬è©¦æª”æ¡ˆæ”¾ç½®åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€ï¼š")
            logger.info("- test_VIDEO/ ç›®éŒ„")
            logger.info("- C:/Users/USER-ART0/Desktop/")
            logger.info("- ç•¶å‰ python/ ç›®éŒ„")
            return False
        
        # 3. åˆ†é¡æª”æ¡ˆ
        categorized_files = dataset_manager.categorize_existing_files(existing_files)
        
        # 4. å‰µå»ºæ¸¬è©¦æ¸…å–®
        manifest = dataset_manager.create_test_dataset_manifest(categorized_files)
        
        # 5. ç”Ÿæˆæ¸¬è©¦è…³æœ¬
        script_files = dataset_manager.generate_performance_test_scripts(manifest)
        
        # 6. ä¿å­˜æ¸…å–®å’Œèªªæ˜
        manifest_file = dataset_manager.save_manifest(manifest)
        readme_file = dataset_manager.create_readme(manifest)
        
        # 7. è¼¸å‡ºæ‘˜è¦
        logger.info("")
        logger.info("=" * 60)
        logger.info("ğŸ‰ ç¶œåˆæ¸¬è©¦æ•¸æ“šé›†å»ºç«‹å®Œæˆï¼")
        logger.info("")
        logger.info(f"ğŸ“ åŸºç¤ç›®éŒ„: {dataset_manager.base_dir}")
        logger.info(f"ğŸ“Š ç¸½æª”æ¡ˆæ•¸: {manifest['total_files']} å€‹")
        logger.info(f"ğŸ“ æ¸¬è©¦æ¸…å–®: {Path(manifest_file).name}")
        logger.info(f"ğŸ“š èªªæ˜æ–‡æª”: {Path(readme_file).name}")
        logger.info(f"ğŸ”§ æ¸¬è©¦è…³æœ¬: {len(script_files)} å€‹")
        logger.info("")
        logger.info("æª”æ¡ˆåˆ†ä½ˆï¼š")
        for category, info in manifest['categories'].items():
            if info['count'] > 0:
                logger.info(f"  {category}: {info['count']} å€‹æª”æ¡ˆ ({info['total_duration']/60:.1f}åˆ†é˜)")
        
        logger.info("")
        logger.info("ğŸš€ å»ºè­°é–‹å§‹æ¸¬è©¦é †åºï¼š")
        logger.info("1. python run_quick_validation_test.py")
        logger.info("2. python run_standard_benchmark_test.py")
        logger.info("3. python run_comprehensive_test_test.py")
        
        return True
        
    except Exception as e:
        logger.error(f"å»ºç«‹æ¸¬è©¦æ•¸æ“šé›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)