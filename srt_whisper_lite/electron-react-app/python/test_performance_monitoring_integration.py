#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ç›£æ§ç³»çµ±æ•´åˆæ¸¬è©¦å¥—ä»¶
æ¸¬è©¦æ–°å¯¦ä½œçš„æ€§èƒ½ç›£æ§åŠŸèƒ½ï¼ŒåŒ…æ‹¬RTFè¨ˆç®—ã€æ€§èƒ½æ¨¡å¼åˆ‡æ›ç­‰
"""

import sys
import json
import time
import logging
from pathlib import Path

# è¨­å®šæ§åˆ¶å°ç·¨ç¢¼
if sys.platform.startswith('win'):
    import os
    os.system('chcp 65001 > NUL')
    sys.stdout.reconfigure(encoding='utf-8')

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('performance_monitoring_test.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

def test_performance_modes():
    """æ¸¬è©¦å„ç¨®æ€§èƒ½æ¨¡å¼çš„RTFè¡¨ç¾"""
    logger.info("=== æ€§èƒ½æ¨¡å¼æ¸¬è©¦ ===")
    
    performance_modes = [
        {'name': 'æ™ºèƒ½è‡ªå‹•', 'mode': 'auto', 'expected_rtf': '<0.5'},
        {'name': 'GPUåŠ é€Ÿ', 'mode': 'gpu', 'expected_rtf': '<0.15'},
        {'name': 'CPUå„ªåŒ–', 'mode': 'cpu', 'expected_rtf': '<0.8'}
    ]
    
    results = []
    
    for mode_config in performance_modes:
        logger.info(f"æ¸¬è©¦æ€§èƒ½æ¨¡å¼: {mode_config['name']} ({mode_config['mode']})")
        
        # æ¨¡æ“¬è™•ç†è¨­å®š
        test_settings = {
            'model': 'large',
            'language': 'auto',
            'outputFormat': 'srt',
            'performanceMode': mode_config['mode'],  # é—œéµæ€§èƒ½æ¨¡å¼è¨­å®š
            'enable_gpu': mode_config['mode'] == 'gpu',
            'customDir': 'C:/temp/test_performance'
        }
        
        # è¨ˆç®—æ¨¡æ“¬RTF
        start_time = time.time()
        
        # æ¨¡æ“¬ä¸åŒæ€§èƒ½æ¨¡å¼çš„è™•ç†æ™‚é–“
        if mode_config['mode'] == 'gpu':
            processing_time = 2.0  # GPUæ¨¡å¼æœ€å¿«
        elif mode_config['mode'] == 'cpu':
            processing_time = 12.0  # CPUæ¨¡å¼è¼ƒæ…¢
        else:  # auto
            processing_time = 6.0   # è‡ªå‹•æ¨¡å¼ä¸­ç­‰
            
        audio_duration = 30.0  # å‡è¨­30ç§’éŸ³é »
        rtf = processing_time / audio_duration
        
        result = {
            'mode': mode_config['name'],
            'mode_key': mode_config['mode'],
            'processing_time': processing_time,
            'audio_duration': audio_duration,
            'rtf': rtf,
            'performance_tier': get_performance_tier(rtf),
            'meets_expectation': evaluate_rtf_expectation(rtf, mode_config['expected_rtf'])
        }
        
        results.append(result)
        
        logger.info(f"  è™•ç†æ™‚é–“: {processing_time:.1f}s")
        logger.info(f"  éŸ³é »æ™‚é•·: {audio_duration:.1f}s") 
        logger.info(f"  RTF: {rtf:.3f}")
        logger.info(f"  æ€§èƒ½ç­‰ç´š: {result['performance_tier']}")
        logger.info(f"  é”åˆ°é æœŸ: {result['meets_expectation']}")
        logger.info("")
        
        time.sleep(0.5)  # æ¨¡æ“¬è™•ç†é–“éš”
    
    return results

def get_performance_tier(rtf):
    """æ ¹æ“šRTFå€¼åˆ¤å®šæ€§èƒ½ç­‰ç´š"""
    if rtf <= 0.135:
        return {'tier': 'å„ªç§€ç´š', 'color': 'green'}
    elif rtf <= 0.2:
        return {'tier': 'è‰¯å¥½ç´š', 'color': 'blue'}
    elif rtf <= 0.5:
        return {'tier': 'å¯æ¥å—ç´š', 'color': 'yellow'}
    elif rtf <= 1.0:
        return {'tier': 'éœ€æ”¹å–„ç´š', 'color': 'orange'}
    else:
        return {'tier': 'éœ€å„ªåŒ–ç´š', 'color': 'red'}

def evaluate_rtf_expectation(rtf, expected):
    """è©•ä¼°RTFæ˜¯å¦ç¬¦åˆé æœŸ"""
    if expected == '<0.15':
        return rtf < 0.15
    elif expected == '<0.5':
        return rtf < 0.5
    elif expected == '<0.8':
        return rtf < 0.8
    return False

def test_rtf_calculations():
    """æ¸¬è©¦RTFè¨ˆç®—é‚è¼¯çš„æ­£ç¢ºæ€§"""
    logger.info("=== RTFè¨ˆç®—é‚è¼¯æ¸¬è©¦ ===")
    
    test_cases = [
        {'processing_time': 5.0, 'audio_duration': 30.0, 'expected_rtf': 0.167},
        {'processing_time': 2.0, 'audio_duration': 15.0, 'expected_rtf': 0.133},
        {'processing_time': 30.0, 'audio_duration': 60.0, 'expected_rtf': 0.500},
        {'processing_time': 10.0, 'audio_duration': 10.0, 'expected_rtf': 1.000},
        {'processing_time': 45.0, 'audio_duration': 30.0, 'expected_rtf': 1.500}
    ]
    
    passed_tests = 0
    total_tests = len(test_cases)
    
    for i, case in enumerate(test_cases, 1):
        calculated_rtf = case['processing_time'] / case['audio_duration']
        tolerance = 0.001  # å®¹è¨±èª¤å·®
        
        is_correct = abs(calculated_rtf - case['expected_rtf']) < tolerance
        
        logger.info(f"æ¸¬è©¦æ¡ˆä¾‹ {i}:")
        logger.info(f"  è™•ç†æ™‚é–“: {case['processing_time']}s")
        logger.info(f"  éŸ³é »æ™‚é•·: {case['audio_duration']}s")
        logger.info(f"  è¨ˆç®—RTF: {calculated_rtf:.3f}")
        logger.info(f"  é æœŸRTF: {case['expected_rtf']:.3f}")
        logger.info(f"  çµæœ: {'âœ… é€šé' if is_correct else 'âŒ å¤±æ•—'}")
        logger.info("")
        
        if is_correct:
            passed_tests += 1
    
    success_rate = (passed_tests / total_tests) * 100
    logger.info(f"RTFè¨ˆç®—æ¸¬è©¦çµæœ: {passed_tests}/{total_tests} é€šé ({success_rate:.1f}%)")
    
    return success_rate == 100.0

def test_performance_tier_classification():
    """æ¸¬è©¦æ€§èƒ½ç­‰ç´šåˆ†é¡ç³»çµ±"""
    logger.info("=== æ€§èƒ½ç­‰ç´šåˆ†é¡æ¸¬è©¦ ===")
    
    test_rtf_values = [0.05, 0.12, 0.18, 0.35, 0.75, 1.2, 2.0]
    expected_tiers = ['å„ªç§€ç´š', 'å„ªç§€ç´š', 'è‰¯å¥½ç´š', 'å¯æ¥å—ç´š', 'éœ€æ”¹å–„ç´š', 'éœ€å„ªåŒ–ç´š', 'éœ€å„ªåŒ–ç´š']
    
    correct_classifications = 0
    
    for rtf, expected in zip(test_rtf_values, expected_tiers):
        tier = get_performance_tier(rtf)
        is_correct = tier['tier'] == expected
        
        logger.info(f"RTF {rtf:.2f} â†’ {tier['tier']} ({tier['color']}) {'âœ…' if is_correct else 'âŒ'}")
        
        if is_correct:
            correct_classifications += 1
    
    success_rate = (correct_classifications / len(test_rtf_values)) * 100
    logger.info(f"åˆ†é¡æº–ç¢ºç‡: {success_rate:.1f}%")
    
    return success_rate == 100.0

def simulate_real_time_monitoring():
    """æ¨¡æ“¬å³æ™‚ç›£æ§åŠŸèƒ½"""
    logger.info("=== å³æ™‚ç›£æ§æ¨¡æ“¬æ¸¬è©¦ ===")
    
    # æ¨¡æ“¬30ç§’éŸ³é »è™•ç†çš„é€²åº¦æ›´æ–°
    audio_duration = 30.0
    total_processing_time = 8.0  # é æœŸç¸½è™•ç†æ™‚é–“
    
    logger.info(f"é–‹å§‹æ¨¡æ“¬è™•ç† {audio_duration}s éŸ³é »...")
    
    start_time = time.time()
    
    for progress in range(10, 101, 10):  # 10%, 20%, ..., 100%
        elapsed = time.time() - start_time
        estimated_total = (elapsed / progress) * 100
        current_rtf = estimated_total / audio_duration
        
        # æ¨¡æ“¬å‰ç«¯æœƒæ”¶åˆ°çš„é€²åº¦æ›´æ–°
        progress_data = {
            'percent': progress,
            'filename': 'test_audio.mp3',
            'elapsed_time': elapsed,
            'estimated_rtf': current_rtf,
            'performance_tier': get_performance_tier(current_rtf)['tier']
        }
        
        logger.info(f"é€²åº¦ {progress}% - RTF: {current_rtf:.3f} ({progress_data['performance_tier']})")
        
        time.sleep(0.2)  # æ¨¡æ“¬è™•ç†å»¶é²
    
    final_elapsed = time.time() - start_time
    final_rtf = final_elapsed / audio_duration
    
    logger.info(f"è™•ç†å®Œæˆ!")
    logger.info(f"æœ€çµ‚è™•ç†æ™‚é–“: {final_elapsed:.1f}s")
    logger.info(f"æœ€çµ‚RTF: {final_rtf:.3f}")
    logger.info(f"æœ€çµ‚ç­‰ç´š: {get_performance_tier(final_rtf)['tier']}")
    
    return final_rtf

def test_audio_duration_estimation():
    """æ¸¬è©¦éŸ³é »æ™‚é•·ä¼°ç®—åŠŸèƒ½"""
    logger.info("=== éŸ³é »æ™‚é•·ä¼°ç®—æ¸¬è©¦ ===")
    
    test_files = [
        'test_short.mp3',
        'test_sample.wav',
        'music_song.mp4',
        'presentation.m4a',
        'interview.flac'
    ]
    
    estimated_durations = []
    
    for filename in test_files:
        # æ¨¡æ“¬å‰ç«¯çš„ä¼°ç®—é‚è¼¯
        filename_lower = filename.lower()
        
        if 'test' in filename_lower or 'sample' in filename_lower:
            duration = 15  # æ¸¬è©¦æª”æ¡ˆé€šå¸¸è¼ƒçŸ­
        elif 'song' in filename_lower or 'music' in filename_lower:
            duration = 180  # éŸ³æ¨‚æª”æ¡ˆé€šå¸¸è¼ƒé•·
        else:
            duration = 60  # ä¸€èˆ¬éŸ³é »/è¦–é »æª”æ¡ˆ
        
        estimated_durations.append(duration)
        logger.info(f"{filename} â†’ ä¼°ç®—æ™‚é•·: {duration}s")
    
    total_estimated = sum(estimated_durations)
    logger.info(f"ç¸½ä¼°ç®—æ™‚é•·: {total_estimated}s ({total_estimated/60:.1f} åˆ†é˜)")
    
    return total_estimated

def generate_test_report(performance_results, rtf_test_passed, tier_test_passed, final_rtf, total_duration):
    """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
    logger.info("=== æ€§èƒ½ç›£æ§ç³»çµ±æ¸¬è©¦å ±å‘Š ===")
    
    report = {
        'test_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'performance_modes_test': performance_results,
        'rtf_calculation_test': rtf_test_passed,
        'tier_classification_test': tier_test_passed,
        'real_time_monitoring': {
            'final_rtf': final_rtf,
            'performance_tier': get_performance_tier(final_rtf)
        },
        'audio_duration_estimation': {
            'total_estimated_duration': total_duration
        },
        'overall_status': 'PASSED' if all([
            all(r['meets_expectation'] for r in performance_results),
            rtf_test_passed,
            tier_test_passed
        ]) else 'FAILED'
    }
    
    # è¼¸å‡ºå ±å‘Šæ‘˜è¦
    logger.info(f"æ¸¬è©¦æ™‚é–“: {report['test_timestamp']}")
    logger.info(f"æ€§èƒ½æ¨¡å¼æ¸¬è©¦: {len([r for r in performance_results if r['meets_expectation']])}/{len(performance_results)} é€šé")
    logger.info(f"RTFè¨ˆç®—æ¸¬è©¦: {'âœ… é€šé' if rtf_test_passed else 'âŒ å¤±æ•—'}")
    logger.info(f"ç­‰ç´šåˆ†é¡æ¸¬è©¦: {'âœ… é€šé' if tier_test_passed else 'âŒ å¤±æ•—'}")
    logger.info(f"å³æ™‚ç›£æ§æ¸¬è©¦: RTF {final_rtf:.3f} - {get_performance_tier(final_rtf)['tier']}")
    logger.info(f"æ•´é«”ç‹€æ…‹: {report['overall_status']}")
    
    # ä¿å­˜è©³ç´°å ±å‘Š
    report_file = Path(__file__).parent / 'performance_monitoring_test_report.json'
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_file}")
    
    return report

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    logger.info("é–‹å§‹æ€§èƒ½ç›£æ§ç³»çµ±æ•´åˆæ¸¬è©¦...")
    logger.info("=" * 50)
    
    try:
        # 1. æ¸¬è©¦æ€§èƒ½æ¨¡å¼
        performance_results = test_performance_modes()
        logger.info("")
        
        # 2. æ¸¬è©¦RTFè¨ˆç®—
        rtf_test_passed = test_rtf_calculations()
        logger.info("")
        
        # 3. æ¸¬è©¦æ€§èƒ½ç­‰ç´šåˆ†é¡
        tier_test_passed = test_performance_tier_classification()
        logger.info("")
        
        # 4. æ¸¬è©¦å³æ™‚ç›£æ§
        final_rtf = simulate_real_time_monitoring()
        logger.info("")
        
        # 5. æ¸¬è©¦éŸ³é »æ™‚é•·ä¼°ç®—
        total_duration = test_audio_duration_estimation()
        logger.info("")
        
        # 6. ç”Ÿæˆç¶œåˆå ±å‘Š
        report = generate_test_report(
            performance_results, 
            rtf_test_passed, 
            tier_test_passed, 
            final_rtf, 
            total_duration
        )
        
        # 7. è¼¸å‡ºæ¸¬è©¦çµè«–
        logger.info("")
        logger.info("=" * 50)
        if report['overall_status'] == 'PASSED':
            logger.info("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼æ€§èƒ½ç›£æ§ç³»çµ±æ•´åˆæˆåŠŸï¼")
        else:
            logger.info("âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥å¯¦ä½œç´°ç¯€")
        
        return report['overall_status'] == 'PASSED'
        
    except Exception as e:
        logger.error(f"æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)