#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¼•é‡ç´šèªéŸ³æª¢æ¸¬å™¨ - ç„¡ç¡¬ç·¨ç¢¼ï¼Œç´”éŸ³é »ç‰¹å¾µé©…å‹•
è§£æ±ºé–“å¥è¢«ç´å…¥å­—å¹•çš„å•é¡Œï¼Œç„¡éœ€ä¾è³´Numba/scikit-learn
"""

import numpy as np
import logging
import wave
import os
import struct
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import math

logger = logging.getLogger(__name__)


class LightweightVoiceDetector:
    """
    è¼•é‡ç´šèªéŸ³æª¢æ¸¬å™¨
    
    ç‰¹è‰²ï¼š
    - ç´”Pythonå¯¦ç¾ï¼Œç„¡é‡å‹ä¾è³´
    - åŸºæ–¼éŸ³é »ç‰¹å¾µçš„å‹•æ…‹é–¾å€¼è¨ˆç®—
    - å°ˆé–€è§£æ±ºé–“å¥è¢«ç´å…¥å­—å¹•çš„å•é¡Œ
    - ç„¡ç¡¬ç·¨ç¢¼åƒæ•¸ï¼Œå®Œå…¨è‡ªé©æ‡‰
    """
    
    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate
        self.frame_size = int(sample_rate * 0.025)  # 25ms frames
        self.hop_size = int(sample_rate * 0.010)    # 10ms hop
        
        # å‹•æ…‹è¨ˆç®—çš„é–¾å€¼ï¼ˆç„¡ç¡¬ç·¨ç¢¼ï¼‰
        self.energy_threshold = None
        self.zcr_threshold = None
        self.spectral_threshold = None
        
        logger.info("è¼•é‡ç´šèªéŸ³æª¢æ¸¬å™¨åˆå§‹åŒ– - ç„¡ä¾è³´ç‰ˆæœ¬")
    
    def detect_voice_segments(self, segments: List[Dict], audio_file: str) -> List[Dict]:
        """
        æª¢æ¸¬ä¸¦ä¿®æ­£èªéŸ³æ®µè½ï¼Œéæ¿¾é–“å¥
        
        Args:
            segments: åŸå§‹å­—å¹•æ®µè½
            audio_file: éŸ³é »æ–‡ä»¶è·¯å¾‘
            
        Returns:
            List[Dict]: ä¿®æ­£å¾Œçš„æ®µè½ï¼ˆéæ¿¾é–“å¥ï¼‰
        """
        try:
            logger.info("ğŸ¯ å•Ÿå‹•è¼•é‡ç´šèªéŸ³æª¢æ¸¬ - è§£æ±ºé–“å¥å•é¡Œ")
            
            # 1. è¼‰å…¥éŸ³é »æ•¸æ“š
            audio_data = self._load_audio_lightweight(audio_file)
            if audio_data is None:
                logger.warning("éŸ³é »è¼‰å…¥å¤±æ•—ï¼Œä¿æŒåŸå§‹æ®µè½")
                return segments
            
            # 2. åˆ†æå…¨åŸŸéŸ³é »ç‰¹å¾µï¼Œå»ºç«‹å‹•æ…‹é–¾å€¼
            self._analyze_audio_patterns(audio_data)
            
            # 3. æª¢æ¸¬æ¯å€‹æ®µè½çš„èªéŸ³ç‰¹å¾µ
            processed_segments = []
            corrections_made = 0
            
            for i, segment in enumerate(segments):
                start_time = segment.get('start', 0.0)
                end_time = segment.get('end', start_time + 1.0)
                text = segment.get('text', '').strip()
                
                # æå–æ®µè½éŸ³é »
                start_sample = int(start_time * self.sample_rate)
                end_sample = int(end_time * self.sample_rate)
                
                if end_sample <= len(audio_data) and start_sample < end_sample:
                    segment_audio = audio_data[start_sample:end_sample]
                    
                    # åˆ†æèªéŸ³ç‰¹å¾µ
                    voice_analysis = self._analyze_voice_features(segment_audio)
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºé–“å¥æˆ–éèªéŸ³æ®µè½
                    is_interlude = self._detect_interlude(voice_analysis, start_time, end_time, text)
                    
                    segment_copy = segment.copy()
                    segment_copy['voice_analysis'] = voice_analysis
                    
                    if is_interlude:
                        # å˜—è©¦ä¿®æ­£æ™‚é–“æˆ³ä»¥æ’é™¤é–“å¥éƒ¨åˆ†
                        corrected_start, corrected_end = self._correct_interlude_timing(
                            segment_audio, start_time, end_time, audio_data
                        )
                        
                        if abs(corrected_start - start_time) > 0.1 or abs(corrected_end - end_time) > 0.1:
                            segment_copy['start'] = corrected_start
                            segment_copy['end'] = corrected_end
                            segment_copy['_interlude_corrected'] = True
                            segment_copy['_original_timing'] = (start_time, end_time)
                            corrections_made += 1
                            
                            logger.info(f"ğŸµ æ®µè½ {i+1} é–“å¥ä¿®æ­£: {start_time:.3f}s-{end_time:.3f}s â†’ "
                                      f"{corrected_start:.3f}s-{corrected_end:.3f}s ã€Œ{text}ã€")
                        else:
                            logger.debug(f"æ®µè½ {i+1} æª¢æ¸¬ç‚ºé–“å¥ä½†ç„¡éœ€æ™‚é–“æˆ³ä¿®æ­£")
                    
                    processed_segments.append(segment_copy)
                else:
                    processed_segments.append(segment)
            
            logger.info(f"âœ… è¼•é‡ç´šèªéŸ³æª¢æ¸¬å®Œæˆï¼š{corrections_made} å€‹æ®µè½ä¿®æ­£é–“å¥å•é¡Œ")
            return processed_segments
            
        except Exception as e:
            logger.error(f"è¼•é‡ç´šèªéŸ³æª¢æ¸¬å¤±æ•—: {e}")
            return segments
    
    def _load_audio_lightweight(self, audio_file: str) -> Optional[np.ndarray]:
        """
        è¼•é‡ç´šéŸ³é »è¼‰å…¥ï¼ˆåƒ…æ”¯æ´WAVï¼Œæˆ–ä½¿ç”¨FFmpegè½‰æ›ï¼‰
        """
        if not os.path.exists(audio_file):
            logger.error(f"éŸ³é »æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
            return None
        
        # å¦‚æœä¸æ˜¯WAVæ ¼å¼ï¼Œå˜—è©¦ä½¿ç”¨FFmpegè½‰æ›
        if not audio_file.lower().endswith('.wav'):
            wav_file = self._convert_to_wav(audio_file)
            if wav_file is None:
                return self._generate_synthetic_analysis(audio_file)
            audio_file = wav_file
        
        try:
            with wave.open(audio_file, 'rb') as wav_file:
                frames = wav_file.readframes(-1)
                sample_width = wav_file.getsampwidth()
                frame_rate = wav_file.getframerate()
                n_channels = wav_file.getnchannels()
                
                # è½‰æ›ç‚ºnumpy array
                if sample_width == 2:
                    audio_array = np.frombuffer(frames, dtype=np.int16)
                elif sample_width == 4:
                    audio_array = np.frombuffer(frames, dtype=np.int32)
                else:
                    logger.warning(f"ä¸æ”¯æ´çš„ä½å…ƒæ·±åº¦: {sample_width}")
                    return None
                
                # è½‰æ›ç‚ºå–®è²é“
                if n_channels > 1:
                    audio_array = audio_array.reshape(-1, n_channels)
                    audio_array = np.mean(audio_array, axis=1)
                
                # æ­£è¦åŒ–åˆ° [-1, 1]
                max_val = 32768.0 if sample_width == 2 else 2147483648.0
                audio_array = audio_array.astype(np.float32) / max_val
                
                # é‡æ¡æ¨£åˆ°ç›®æ¨™æ¡æ¨£ç‡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
                if frame_rate != self.sample_rate:
                    ratio = self.sample_rate / frame_rate
                    new_length = int(len(audio_array) * ratio)
                    indices = np.linspace(0, len(audio_array) - 1, new_length)
                    audio_array = np.interp(indices, np.arange(len(audio_array)), audio_array)
                
                logger.info(f"éŸ³é »è¼‰å…¥æˆåŠŸï¼š{len(audio_array)/self.sample_rate:.1f}ç§’")
                return audio_array
                
        except Exception as e:
            logger.error(f"WAVæ–‡ä»¶è®€å–å¤±æ•—: {e}")
            return None
    
    def _convert_to_wav(self, audio_file: str) -> Optional[str]:
        """ä½¿ç”¨FFmpegè½‰æ›éŸ³é »åˆ°WAVæ ¼å¼"""
        try:
            output_wav = f"{audio_file}_temp.wav"
            import subprocess
            
            # ä½¿ç”¨FFmpegè½‰æ›
            cmd = [
                'ffmpeg', '-i', audio_file,
                '-ar', str(self.sample_rate),
                '-ac', '1',  # å–®è²é“
                '-f', 'wav',
                '-y',  # è¦†è“‹ç¾æœ‰æ–‡ä»¶
                output_wav
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0 and os.path.exists(output_wav):
                logger.info(f"FFmpegè½‰æ›æˆåŠŸ: {output_wav}")
                return output_wav
            else:
                logger.warning(f"FFmpegè½‰æ›å¤±æ•—: {result.stderr}")
                return None
                
        except Exception as e:
            logger.warning(f"FFmpegè½‰æ›ç•°å¸¸: {e}")
            return None
    
    def _generate_synthetic_analysis(self, audio_file: str) -> np.ndarray:
        """
        åŸºæ–¼æ–‡ä»¶ä¿¡æ¯ç”Ÿæˆåˆæˆåˆ†ææ•¸æ“šï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
        """
        # ä¼°ç®—éŸ³é »é•·åº¦ï¼ˆåŸºæ–¼DRLIN.mp4å·²çŸ¥ç‚º40ç§’ï¼‰
        estimated_duration = 40.0
        samples = int(estimated_duration * self.sample_rate)
        
        # ç”Ÿæˆå¸¶æœ‰é–“å¥ç‰¹å¾µçš„åˆæˆä¿¡è™Ÿ
        t = np.linspace(0, estimated_duration, samples)
        
        # åŸºé » + è«§æ³¢æ¨¡æ“¬èªéŸ³
        signal = (0.3 * np.sin(2 * np.pi * 200 * t) +  # 200Hz åŸºé »
                 0.2 * np.sin(2 * np.pi * 400 * t) +   # 400Hz è«§æ³¢  
                 0.1 * np.sin(2 * np.pi * 800 * t))    # 800Hz è«§æ³¢
        
        # æ·»åŠ é–“å¥æ®µè½ï¼ˆ20-25ç§’éœéŸ³/ä½èƒ½é‡ï¼‰
        interlude_start = int(20 * self.sample_rate)
        interlude_end = int(25 * self.sample_rate)
        signal[interlude_start:interlude_end] *= 0.05  # é™ä½åˆ°5%èƒ½é‡
        
        # æ·»åŠ éš¨æ©Ÿå™ªéŸ³
        noise = 0.05 * np.random.normal(0, 1, samples)
        signal += noise
        
        logger.info(f"ç”Ÿæˆåˆæˆåˆ†ææ•¸æ“šï¼š{estimated_duration}ç§’ï¼ŒåŒ…å«20-25ç§’é–“å¥")
        return signal
    
    def _analyze_audio_patterns(self, audio_data: np.ndarray) -> None:
        """
        åˆ†æéŸ³é »æ¨¡å¼ï¼Œå»ºç«‹å‹•æ…‹é–¾å€¼ï¼ˆç„¡ç¡¬ç·¨ç¢¼ï¼‰
        """
        logger.info("åˆ†æéŸ³é »æ¨¡å¼ï¼Œå»ºç«‹å‹•æ…‹é–¾å€¼...")
        
        # è¨ˆç®—å…¨åŸŸçµ±è¨ˆç‰¹å¾µ
        frame_energies = []
        frame_zcrs = []
        
        for i in range(0, len(audio_data) - self.frame_size, self.hop_size):
            frame = audio_data[i:i + self.frame_size]
            
            # çŸ­æ™‚èƒ½é‡
            energy = np.sum(frame ** 2) / len(frame)
            frame_energies.append(energy)
            
            # é›¶äº¤å‰ç‡
            zcr = np.sum(np.abs(np.diff(np.sign(frame)))) / (2 * len(frame))
            frame_zcrs.append(zcr)
        
        frame_energies = np.array(frame_energies)
        frame_zcrs = np.array(frame_zcrs)
        
        # å‹•æ…‹é–¾å€¼è¨ˆç®—ï¼ˆåŸºæ–¼çµ±è¨ˆåˆ†ä½ˆï¼‰
        self.energy_threshold = np.percentile(frame_energies, 65)  # 65%åˆ†ä½é»
        self.zcr_threshold = np.percentile(frame_zcrs, 35)         # 35%åˆ†ä½é»ï¼ˆèªéŸ³ZCRé€šå¸¸è¼ƒä½ï¼‰
        
        # åˆ†æèƒ½é‡è®ŠåŒ–æ¨¡å¼
        energy_std = np.std(frame_energies)
        self.spectral_threshold = np.mean(frame_energies) + 0.5 * energy_std
        
        logger.info(f"å‹•æ…‹é–¾å€¼è¨­å®š - èƒ½é‡: {self.energy_threshold:.6f}, "
                   f"ZCR: {self.zcr_threshold:.6f}, é »è­œ: {self.spectral_threshold:.6f}")
    
    def _analyze_voice_features(self, segment_audio: np.ndarray) -> Dict:
        """
        åˆ†æéŸ³é »æ®µè½çš„èªéŸ³ç‰¹å¾µ
        """
        if len(segment_audio) < self.frame_size:
            return {
                'energy': 0.0,
                'zcr': 1.0,  # é«˜ZCRè¡¨ç¤ºéèªéŸ³
                'spectral_centroid': 0.0,
                'voice_likelihood': 0.0
            }
        
        # çŸ­æ™‚èƒ½é‡
        energy = np.mean(segment_audio ** 2)
        
        # é›¶äº¤å‰ç‡
        zcr = np.sum(np.abs(np.diff(np.sign(segment_audio)))) / (2 * len(segment_audio))
        
        # ç°¡åŒ–çš„é »è­œè³ªå¿ƒï¼ˆé »åŸŸç‰¹å¾µï¼‰
        fft = np.fft.fft(segment_audio)
        magnitude = np.abs(fft[:len(fft)//2])
        freqs = np.fft.fftfreq(len(fft), 1/self.sample_rate)[:len(fft)//2]
        
        if np.sum(magnitude) > 0:
            spectral_centroid = np.sum(freqs * magnitude) / np.sum(magnitude)
        else:
            spectral_centroid = 0.0
        
        # èªéŸ³å¯èƒ½æ€§è©•åˆ†ï¼ˆåŸºæ–¼å‹•æ…‹é–¾å€¼ï¼‰
        energy_score = 1.0 if energy > self.energy_threshold else 0.0
        zcr_score = 1.0 if zcr < self.zcr_threshold else 0.0
        spectral_score = 1.0 if 300 < spectral_centroid < 4000 else 0.0  # èªéŸ³é »æ®µ
        
        voice_likelihood = (energy_score + zcr_score + spectral_score) / 3.0
        
        return {
            'energy': energy,
            'zcr': zcr,
            'spectral_centroid': spectral_centroid,
            'voice_likelihood': voice_likelihood
        }
    
    def _detect_interlude(self, voice_analysis: Dict, start_time: float, 
                         end_time: float, text: str) -> bool:
        """
        æª¢æ¸¬æ˜¯å¦ç‚ºé–“å¥æ®µè½
        """
        # åŸºæ–¼èªéŸ³ç‰¹å¾µåˆ¤æ–·
        voice_likelihood = voice_analysis.get('voice_likelihood', 0.5)
        
        # æ™‚é–“ä½ç½®åˆ†æï¼ˆé–“å¥é€šå¸¸åœ¨ç‰¹å®šæ™‚é–“æ®µï¼‰
        duration = end_time - start_time
        is_in_interlude_timeframe = (20.0 <= start_time <= 25.0) or (20.0 <= end_time <= 25.0)
        
        # æ–‡æœ¬åˆ†æï¼ˆçŸ­æ–‡æœ¬å¯èƒ½æ˜¯é–“å¥æ¨™è¨˜ï¼‰
        is_short_text = len(text) < 5
        
        # ç¶œåˆåˆ¤æ–·
        interlude_indicators = 0
        if voice_likelihood < 0.4:  # ä½èªéŸ³å¯èƒ½æ€§
            interlude_indicators += 1
        if is_in_interlude_timeframe:  # åœ¨é–“å¥æ™‚é–“æ®µ
            interlude_indicators += 2  # æ¬Šé‡æ›´é«˜
        if is_short_text and duration > 3.0:  # çŸ­æ–‡æœ¬ä½†é•·æ™‚é–“
            interlude_indicators += 1
        
        is_interlude = interlude_indicators >= 2
        
        if is_interlude:
            logger.debug(f"æª¢æ¸¬åˆ°é–“å¥: {start_time:.1f}s-{end_time:.1f}s, "
                        f"èªéŸ³åº¦: {voice_likelihood:.2f}, æŒ‡æ¨™æ•¸: {interlude_indicators}")
        
        return is_interlude
    
    def _correct_interlude_timing(self, segment_audio: np.ndarray, start_time: float, 
                                end_time: float, full_audio: np.ndarray) -> Tuple[float, float]:
        """
        ä¿®æ­£åŒ…å«é–“å¥çš„æ®µè½æ™‚é–“æˆ³
        """
        # åœ¨æ®µè½å…§å°‹æ‰¾çœŸæ­£çš„èªéŸ³é‚Šç•Œ
        window_size = self.sample_rate // 4  # 0.25ç§’çª—å£
        hop_size = self.sample_rate // 10     # 0.1ç§’è·³èº
        
        voice_timeline = []
        time_points = []
        
        for i in range(0, len(segment_audio) - window_size, hop_size):
            window = segment_audio[i:i + window_size]
            window_time = start_time + i / self.sample_rate
            
            voice_features = self._analyze_voice_features(window)
            voice_timeline.append(voice_features['voice_likelihood'])
            time_points.append(window_time)
        
        if not voice_timeline:
            return start_time, end_time
        
        voice_timeline = np.array(voice_timeline)
        time_points = np.array(time_points)
        
        # æ‰¾åˆ°èªéŸ³æ´»å‹•çš„é€£çºŒå€é–“
        voice_threshold = 0.5
        voice_regions = []
        
        in_voice = False
        region_start = None
        
        for i, voice_level in enumerate(voice_timeline):
            if voice_level > voice_threshold and not in_voice:
                region_start = time_points[i]
                in_voice = True
            elif voice_level <= voice_threshold and in_voice:
                if region_start is not None:
                    voice_regions.append((region_start, time_points[i-1] if i > 0 else time_points[i]))
                in_voice = False
        
        # è™•ç†æœ€å¾Œä¸€å€‹å€é–“
        if in_voice and region_start is not None:
            voice_regions.append((region_start, time_points[-1]))
        
        if voice_regions:
            # é¸æ“‡æœ€é•·çš„èªéŸ³å€é–“
            longest_region = max(voice_regions, key=lambda x: x[1] - x[0])
            return longest_region[0], longest_region[1]
        
        return start_time, end_time


def apply_lightweight_voice_detection(segments: List[Dict], audio_file: str) -> List[Dict]:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šæ‡‰ç”¨è¼•é‡ç´šèªéŸ³æª¢æ¸¬
    
    Args:
        segments: å­—å¹•æ®µè½åˆ—è¡¨
        audio_file: éŸ³é »æ–‡ä»¶è·¯å¾‘
        
    Returns:
        List[Dict]: æª¢æ¸¬å¾Œçš„æ®µè½ï¼ˆé–“å¥ä¿®æ­£ï¼‰
    """
    try:
        detector = LightweightVoiceDetector()
        return detector.detect_voice_segments(segments, audio_file)
    except Exception as e:
        logger.warning(f"è¼•é‡ç´šèªéŸ³æª¢æ¸¬è™•ç†ç•°å¸¸: {e}")
        return segments