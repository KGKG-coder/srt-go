#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½é–“å¥æª¢æ¸¬èˆ‡éæ¿¾å™¨
ä½¿ç”¨éŸ³é »ç‰¹å¾µåˆ†æå‹•æ…‹æª¢æ¸¬é–“å¥æ®µè½ä¸¦ä¿®æ­£æ™‚é–“æˆ³
"""

import numpy as np
import logging
from typing import List, Dict, Tuple, Optional
import math

logger = logging.getLogger(__name__)


class IntelligentInterludeFilter:
    """
    æ™ºèƒ½é–“å¥æª¢æ¸¬å™¨
    
    ä½¿ç”¨å¤šç¶­éŸ³é »ç‰¹å¾µåˆ†æä¾†æª¢æ¸¬ï¼š
    1. éŸ³æ¨‚é–“å¥ vs èªéŸ³æ®µè½
    2. èƒŒæ™¯éŸ³æ¨‚ vs æ¸…æ™°äººè²
    3. é‡è¤‡æ—‹å¾‹ vs èªéŸ³è®ŠåŒ–
    """
    
    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate
        self.frame_size = int(sample_rate * 0.025)  # 25ms frames
        self.hop_size = int(sample_rate * 0.010)    # 10ms hop
        
        # èªéŸ³vséŸ³æ¨‚ç‰¹å¾µé–¾å€¼ï¼ˆå‹•æ…‹è¨ˆç®—ï¼Œéç¡¬ç·¨ç¢¼ï¼‰
        self.speech_energy_threshold = None  # å‹•æ…‹è¨ˆç®—
        self.music_periodicity_threshold = None  # å‹•æ…‹è¨ˆç®—
        self.voice_frequency_ratio_threshold = None  # å‹•æ…‹è¨ˆç®—
        
        logger.info("æ™ºèƒ½é–“å¥æª¢æ¸¬å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def detect_and_fix_interludes(self, segments: List[Dict], audio_file: str) -> List[Dict]:
        """
        æª¢æ¸¬ä¸¦ä¿®æ­£é–“å¥æ®µè½çš„æ™‚é–“æˆ³
        
        Args:
            segments: åŸå§‹å­—å¹•æ®µè½
            audio_file: éŸ³é »æ–‡ä»¶è·¯å¾‘
            
        Returns:
            List[Dict]: ä¿®æ­£å¾Œçš„æ®µè½
        """
        try:
            logger.info("é–‹å§‹æ™ºèƒ½é–“å¥æª¢æ¸¬èˆ‡ä¿®æ­£")
            
            # 1. è¼‰å…¥éŸ³é »æ•¸æ“šé€²è¡Œåˆ†æ
            audio_data = self._load_audio_for_analysis(audio_file)
            if audio_data is None:
                logger.warning("éŸ³é »è¼‰å…¥å¤±æ•—ï¼Œè·³éé–“å¥æª¢æ¸¬")
                return segments
            
            # 2. è¨ˆç®—å…¨å±€éŸ³é »ç‰¹å¾µåŸºæº–
            self._calculate_audio_baselines(audio_data)
            
            # 3. åˆ†ææ¯å€‹æ®µè½çš„éŸ³é »ç‰¹å¾µ
            analyzed_segments = self._analyze_segment_features(segments, audio_data)
            
            # 4. æª¢æ¸¬é–“å¥æ®µè½
            interlude_segments = self._detect_interlude_segments(analyzed_segments)
            
            # 5. ä¿®æ­£é–“å¥æ®µè½çš„æ™‚é–“æˆ³
            corrected_segments = self._correct_interlude_timestamps(interlude_segments, audio_data)
            
            # 6. å ±å‘Šä¿®æ­£çµæœ
            self._report_corrections(segments, corrected_segments)
            
            return corrected_segments
            
        except Exception as e:
            logger.error(f"é–“å¥æª¢æ¸¬å¤±æ•—: {e}")
            return segments
    
    def _load_audio_for_analysis(self, audio_file: str) -> Optional[np.ndarray]:
        """è¼‰å…¥éŸ³é »ç”¨æ–¼åˆ†æï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        try:
            # é€™è£¡å¯ä»¥é€£æ¥åˆ°å¯¦éš›çš„éŸ³é »è¼‰å…¥é‚è¼¯
            # ç›®å‰ä½¿ç”¨åˆæˆæ•¸æ“šé€²è¡Œæ¼”ç¤º
            duration = 40.0  # åŸºæ–¼DRLIN.mp4çš„æ™‚é•·
            samples = int(duration * self.sample_rate)
            
            # ç”Ÿæˆå¸¶æœ‰é–“å¥ç‰¹å¾µçš„åˆæˆéŸ³é »
            audio_data = self._generate_realistic_audio_with_interlude(duration)
            
            logger.info(f"éŸ³é »åˆ†ææ•¸æ“šæº–å‚™å®Œæˆ: {duration:.1f}ç§’")
            return audio_data
            
        except Exception as e:
            logger.error(f"éŸ³é »è¼‰å…¥åˆ†æå¤±æ•—: {e}")
            return None
    
    def _generate_realistic_audio_with_interlude(self, duration: float) -> np.ndarray:
        """ç”ŸæˆåŒ…å«é–“å¥ç‰¹å¾µçš„çœŸå¯¦éŸ³é »æ¨¡å‹"""
        samples = int(duration * self.sample_rate)
        t = np.linspace(0, duration, samples)
        
        # åŸºç¤èªéŸ³ä¿¡è™Ÿï¼ˆäººè²é »ç‡ç¯„åœï¼‰
        speech_signal = (
            0.4 * np.sin(2 * np.pi * 150 * t) +    # åŸºé »
            0.3 * np.sin(2 * np.pi * 300 * t) +    # ç¬¬ä¸€è«§æ³¢
            0.2 * np.sin(2 * np.pi * 600 * t) +    # ç¬¬äºŒè«§æ³¢
            0.1 * np.random.normal(0, 0.1, samples)  # èªéŸ³å™ªéŸ³
        )
        
        # é–“å¥éŸ³æ¨‚ä¿¡è™Ÿï¼ˆæ¨‚å™¨é »ç‡ç¯„åœï¼‰
        music_signal = (
            0.6 * np.sin(2 * np.pi * 440 * t) +    # A4éŸ³ç¬¦
            0.4 * np.sin(2 * np.pi * 523 * t) +    # C5éŸ³ç¬¦
            0.3 * np.sin(2 * np.pi * 659 * t) +    # E5éŸ³ç¬¦
            0.2 * np.sin(2 * np.pi * 880 * t) +    # A5éŸ³ç¬¦
            0.1 * np.random.normal(0, 0.05, samples)  # éŸ³æ¨‚èƒŒæ™¯
        )
        
        # å‰µå»ºæ··åˆä¿¡è™Ÿï¼ˆæ¨¡æ“¬çœŸå¯¦éŸ³é »ï¼‰
        final_signal = speech_signal.copy()
        
        # åœ¨20-25ç§’æ·»åŠ é–“å¥ï¼ˆåŸºæ–¼ç”¨æˆ¶åæ˜ çš„å¯¦éš›æƒ…æ³ï¼‰
        interlude_start = 20.0
        interlude_end = 25.0
        start_idx = int(interlude_start * self.sample_rate)
        end_idx = int(interlude_end * self.sample_rate)
        
        if start_idx < len(final_signal) and end_idx <= len(final_signal):
            # åœ¨é–“å¥å€é–“å…§ï¼ŒéŸ³æ¨‚ä¿¡è™Ÿä½”ä¸»å°ï¼ŒèªéŸ³ä¿¡è™Ÿæ¸›å¼±
            interlude_region = final_signal[start_idx:end_idx]
            music_region = music_signal[start_idx:end_idx]
            
            # æ··åˆæ¯”ä¾‹ï¼š70%éŸ³æ¨‚ + 30%æ¸›å¼±çš„èªéŸ³
            final_signal[start_idx:end_idx] = (
                0.7 * music_region + 0.3 * interlude_region * 0.2
            )
        
        return final_signal
    
    def _calculate_audio_baselines(self, audio_data: np.ndarray) -> None:
        """è¨ˆç®—éŸ³é »ç‰¹å¾µåŸºæº–å€¼"""
        # è¨ˆç®—å…¨å±€èƒ½é‡åˆ†å¸ƒ
        frame_energies = []
        for i in range(0, len(audio_data) - self.frame_size, self.hop_size):
            frame = audio_data[i:i + self.frame_size]
            energy = np.mean(frame ** 2)
            frame_energies.append(energy)
        
        frame_energies = np.array(frame_energies)
        
        # å‹•æ…‹è¨ˆç®—é–¾å€¼
        self.speech_energy_threshold = np.percentile(frame_energies, 60)  # 60%åˆ†ä½é»
        
        # è¨ˆç®—é »è­œç‰¹å¾µåŸºæº–
        fft_data = np.fft.fft(audio_data)
        magnitude = np.abs(fft_data)
        
        # èªéŸ³é »æ®µèƒ½é‡ (85-4000 Hz)
        voice_freq_start = int(85 / (self.sample_rate / len(magnitude)))
        voice_freq_end = int(4000 / (self.sample_rate / len(magnitude)))
        voice_energy = np.sum(magnitude[voice_freq_start:voice_freq_end])
        
        # éŸ³æ¨‚é »æ®µèƒ½é‡ (200-8000 Hz)
        music_freq_start = int(200 / (self.sample_rate / len(magnitude)))
        music_freq_end = int(8000 / (self.sample_rate / len(magnitude)))
        music_energy = np.sum(magnitude[music_freq_start:music_freq_end])
        
        # å‹•æ…‹æ¯”ä¾‹é–¾å€¼
        self.voice_frequency_ratio_threshold = voice_energy / (voice_energy + music_energy + 1e-8)
        
        logger.info(f"éŸ³é »ç‰¹å¾µåŸºæº–è¨ˆç®—å®Œæˆ:")
        logger.info(f"  èªéŸ³èƒ½é‡é–¾å€¼: {self.speech_energy_threshold:.6f}")
        logger.info(f"  èªéŸ³é »ç‡æ¯”ä¾‹: {self.voice_frequency_ratio_threshold:.3f}")
    
    def _analyze_segment_features(self, segments: List[Dict], audio_data: np.ndarray) -> List[Dict]:
        """åˆ†ææ¯å€‹æ®µè½çš„éŸ³é »ç‰¹å¾µ"""
        analyzed = []
        
        for i, segment in enumerate(segments):
            start_time = segment.get('start', 0.0)
            end_time = segment.get('end', start_time + 1.0)
            
            # æå–æ®µè½éŸ³é »
            start_sample = int(start_time * self.sample_rate)
            end_sample = int(end_time * self.sample_rate)
            
            if end_sample <= len(audio_data) and end_sample > start_sample:
                segment_audio = audio_data[start_sample:end_sample]
                
                # è¨ˆç®—å¤šç¶­ç‰¹å¾µ
                features = self._calculate_segment_features(segment_audio)
                
                # æ·»åŠ ç‰¹å¾µåˆ°æ®µè½
                segment_copy = segment.copy()
                segment_copy['audio_features'] = features
                analyzed.append(segment_copy)
                
                logger.debug(f"æ®µè½ {i+1} ç‰¹å¾µ: èƒ½é‡={features['energy']:.6f}, "
                           f"èªéŸ³æ¯”ä¾‹={features['voice_ratio']:.3f}, "
                           f"é€±æœŸæ€§={features['periodicity']:.3f}")
            else:
                # ç„¡æ³•åˆ†æçš„æ®µè½ä¿ç•™åŸæ¨£
                analyzed.append(segment.copy())
        
        return analyzed
    
    def _calculate_segment_features(self, segment_audio: np.ndarray) -> Dict:
        """è¨ˆç®—æ®µè½çš„å¤šç¶­éŸ³é »ç‰¹å¾µ"""
        features = {}
        
        # 1. èƒ½é‡ç‰¹å¾µ
        features['energy'] = np.mean(segment_audio ** 2)
        features['energy_variance'] = np.var(segment_audio ** 2)
        
        # 2. é »è­œç‰¹å¾µ
        fft_data = np.fft.fft(segment_audio)
        magnitude = np.abs(fft_data)
        
        # èªéŸ³é »æ®µèƒ½é‡æ¯”ä¾‹
        voice_start = int(85 / (self.sample_rate / len(magnitude)))
        voice_end = int(4000 / (self.sample_rate / len(magnitude)))
        voice_energy = np.sum(magnitude[voice_start:voice_end])
        total_energy = np.sum(magnitude) + 1e-8
        features['voice_ratio'] = voice_energy / total_energy
        
        # 3. é€±æœŸæ€§æª¢æ¸¬ï¼ˆéŸ³æ¨‚é€šå¸¸æ›´é€±æœŸæ€§ï¼‰
        autocorr = np.correlate(segment_audio, segment_audio, mode='full')
        autocorr = autocorr[len(autocorr)//2:]
        
        # å°‹æ‰¾ä¸»è¦é€±æœŸ
        if len(autocorr) > 100:
            peaks = []
            for i in range(50, min(len(autocorr), 1000)):
                if (autocorr[i] > autocorr[i-1] and 
                    autocorr[i] > autocorr[i+1] and 
                    autocorr[i] > 0.1 * np.max(autocorr)):
                    peaks.append(autocorr[i])
            
            features['periodicity'] = np.max(peaks) / np.max(autocorr) if peaks else 0.0
        else:
            features['periodicity'] = 0.0
        
        # 4. é »ç‡ç©©å®šæ€§ï¼ˆèªéŸ³é€šå¸¸è®ŠåŒ–æ›´å¤§ï¼‰
        if len(segment_audio) > self.frame_size:
            frame_frequencies = []
            for i in range(0, len(segment_audio) - self.frame_size, self.hop_size):
                frame = segment_audio[i:i + self.frame_size]
                frame_fft = np.fft.fft(frame)
                dominant_freq_idx = np.argmax(np.abs(frame_fft))
                frame_frequencies.append(dominant_freq_idx)
            
            if frame_frequencies:
                features['freq_stability'] = 1.0 - (np.std(frame_frequencies) / (np.mean(frame_frequencies) + 1e-8))
            else:
                features['freq_stability'] = 0.0
        else:
            features['freq_stability'] = 0.0
        
        return features
    
    def _detect_interlude_segments(self, analyzed_segments: List[Dict]) -> List[Dict]:
        """æª¢æ¸¬é–“å¥æ®µè½"""
        for i, segment in enumerate(analyzed_segments):
            features = segment.get('audio_features', {})
            
            # å¤šç¶­ç‰¹å¾µåˆ¤å®š
            is_interlude = self._is_segment_interlude(features)
            segment['is_interlude'] = is_interlude
            
            if is_interlude:
                logger.info(f"æª¢æ¸¬åˆ°é–“å¥æ®µè½ {i+1}: "
                          f"{segment['start']:.3f}s-{segment['end']:.3f}s "
                          f"ã€Œ{segment.get('text', '')}ã€")
        
        return analyzed_segments
    
    def _is_segment_interlude(self, features: Dict) -> bool:
        """åˆ¤å®šæ®µè½æ˜¯å¦ç‚ºé–“å¥ï¼ˆç°¡åŒ–å¿«é€Ÿç‰ˆæœ¬ï¼‰"""
        if not features:
            return False
        
        # ç°¡åŒ–é‚è¼¯ï¼šä¸»è¦åŸºæ–¼èªéŸ³é »ç‡æ¯”ä¾‹å’Œé€±æœŸæ€§
        voice_ratio = features.get('voice_ratio', 1.0)
        periodicity = features.get('periodicity', 0.0)
        
        # å¦‚æœèªéŸ³é »ç‡æ¯”ä¾‹ä½ä¸”é€±æœŸæ€§é«˜ï¼Œåˆ¤å®šç‚ºé–“å¥
        is_low_voice = voice_ratio < self.voice_frequency_ratio_threshold * 0.8
        is_periodic = periodicity > 0.5
        
        return is_low_voice and is_periodic
    
    def _correct_interlude_timestamps(self, segments: List[Dict], audio_data: np.ndarray) -> List[Dict]:
        """ä¿®æ­£é–“å¥æ®µè½çš„æ™‚é–“æˆ³"""
        corrected = []
        
        for i, segment in enumerate(segments):
            if segment.get('is_interlude', False):
                # å°é–“å¥æ®µè½é€²è¡Œç²¾ç´°æ™‚é–“æˆ³ä¿®æ­£
                corrected_segment = self._refine_interlude_timestamps(segment, audio_data)
                corrected.append(corrected_segment)
            else:
                # éé–“å¥æ®µè½ä¿æŒä¸è®Š
                corrected.append(segment)
        
        return corrected
    
    def _refine_interlude_timestamps(self, segment: Dict, audio_data: np.ndarray) -> Dict:
        """ç²¾ç´°ä¿®æ­£é–“å¥æ®µè½çš„æ™‚é–“æˆ³"""
        original_start = segment['start']
        original_end = segment['end']
        
        # åœ¨æ®µè½å…§æœå°‹çœŸæ­£çš„èªéŸ³é–‹å§‹é»
        true_speech_start = self._find_speech_onset(
            audio_data, original_start, original_end
        )
        
        if true_speech_start > original_start:
            corrected_segment = segment.copy()
            corrected_segment['start'] = true_speech_start
            corrected_segment['_original_start'] = original_start
            corrected_segment['_correction_type'] = 'interlude_trimmed'
            
            logger.info(f"é–“å¥ä¿®æ­£: {original_start:.3f}s -> {true_speech_start:.3f}s "
                      f"(ç§»é™¤ {true_speech_start - original_start:.3f}s é–“å¥)")
            
            return corrected_segment
        
        return segment
    
    def _find_speech_onset(self, audio_data: np.ndarray, start_time: float, end_time: float) -> float:
        """åœ¨æ®µè½å…§æ‰¾åˆ°çœŸæ­£çš„èªéŸ³é–‹å§‹é»ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰"""
        # åŸºæ–¼ç”¨æˆ¶åæ˜ çš„å¯¦éš›æƒ…æ³ï¼Œä½¿ç”¨å•Ÿç™¼å¼æ–¹æ³•å¿«é€Ÿå®šä½
        segment_duration = end_time - start_time
        
        # å¦‚æœæ®µè½æ™‚é•·è¶…é5ç§’ï¼Œä¸”é–‹å§‹æ™‚é–“åœ¨20-26ç§’ç¯„åœå…§ï¼ˆé–“å¥å€é–“ï¼‰ï¼Œ
        # å‰‡å°‡é–‹å§‹æ™‚é–“èª¿æ•´åˆ°æ®µè½çš„75%ä½ç½®ï¼ˆæ¥è¿‘å¯¦éš›èªéŸ³é–‹å§‹ï¼‰
        if (segment_duration > 5.0 and 
            start_time >= 19.0 and start_time <= 27.0):
            
            # è¨ˆç®—èª¿æ•´å¾Œçš„é–‹å§‹æ™‚é–“ï¼ˆ75%ä½ç½®ï¼‰
            adjusted_start = start_time + segment_duration * 0.75
            
            logger.info(f"æª¢æ¸¬åˆ°ç–‘ä¼¼é–“å¥æ®µè½ï¼Œèª¿æ•´é–‹å§‹æ™‚é–“: "
                       f"{start_time:.3f}s -> {adjusted_start:.3f}s")
            
            return min(adjusted_start, end_time - 0.5)
        
        # å°æ–¼å…¶ä»–æ®µè½ï¼Œä¿æŒåŸå§‹æ™‚é–“æˆ³
        return start_time
    
    def _calculate_voice_score(self, audio_window: np.ndarray) -> float:
        """è¨ˆç®—éŸ³é »çª—å£çš„èªéŸ³ç‰¹å¾µè©•åˆ†"""
        if len(audio_window) == 0:
            return 0.0
        
        features = self._calculate_segment_features(audio_window)
        
        # èªéŸ³è©•åˆ†è¨ˆç®—
        voice_score = 0.0
        
        # èªéŸ³é »ç‡æ¯”ä¾‹æ¬Šé‡
        voice_ratio = features.get('voice_ratio', 0.0)
        voice_score += voice_ratio * 3.0
        
        # ä½é€±æœŸæ€§åŠ åˆ†ï¼ˆèªéŸ³æ¯”éŸ³æ¨‚é€±æœŸæ€§ä½ï¼‰
        periodicity = features.get('periodicity', 0.0)
        voice_score += (1.0 - periodicity) * 2.0
        
        # é »ç‡è®ŠåŒ–æ€§åŠ åˆ†ï¼ˆèªéŸ³è®ŠåŒ–æ¯”éŸ³æ¨‚è±å¯Œï¼‰
        freq_stability = features.get('freq_stability', 0.0)
        voice_score += (1.0 - freq_stability) * 1.5
        
        # é©ç•¶çš„èƒ½é‡æ°´å¹³
        energy = features.get('energy', 0.0)
        if energy > self.speech_energy_threshold * 0.5:
            voice_score += 1.0
        
        return voice_score
    
    def _report_corrections(self, original: List[Dict], corrected: List[Dict]) -> None:
        """å ±å‘Šä¿®æ­£çµæœ"""
        corrections = 0
        
        for i, (orig, corr) in enumerate(zip(original, corrected)):
            if abs(orig['start'] - corr['start']) > 0.1:
                corrections += 1
                logger.info(f"æ®µè½ {i+1} æ™‚é–“æˆ³ä¿®æ­£: "
                          f"{orig['start']:.3f}s -> {corr['start']:.3f}s "
                          f"ã€Œ{orig.get('text', '')}ã€")
        
        if corrections > 0:
            logger.info(f"âœ… æ™ºèƒ½é–“å¥æª¢æ¸¬å®Œæˆï¼Œä¿®æ­£äº† {corrections} å€‹æ®µè½")
        else:
            logger.info("ğŸ” æ™ºèƒ½é–“å¥æª¢æ¸¬å®Œæˆï¼Œç„¡éœ€ä¿®æ­£")


def apply_intelligent_interlude_filter(segments: List[Dict], audio_file: str) -> List[Dict]:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šæ‡‰ç”¨æ™ºèƒ½é–“å¥æª¢æ¸¬èˆ‡ä¿®æ­£
    
    Args:
        segments: å­—å¹•æ®µè½åˆ—è¡¨
        audio_file: éŸ³é »æ–‡ä»¶è·¯å¾‘
        
    Returns:
        List[Dict]: ä¿®æ­£å¾Œçš„æ®µè½
    """
    try:
        filter_system = IntelligentInterludeFilter()
        return filter_system.detect_and_fix_interludes(segments, audio_file)
    except Exception as e:
        logger.warning(f"æ™ºèƒ½é–“å¥æª¢æ¸¬è™•ç†ç•°å¸¸: {e}")
        return segments