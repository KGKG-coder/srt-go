# å•†ç”¨ç´šæ¸¬è©¦æ¶æ§‹ - SRT GO v2.2.1

## ğŸ“‹ æ¸¬è©¦ç­–ç•¥ç¸½è¦½

### æ¸¬è©¦é‡‘å­—å¡”æ¶æ§‹
```
         â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
         â•‘   E2E Tests   â•‘  10% - ç«¯åˆ°ç«¯æ¸¬è©¦
         â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
       â•‘ Integration Tests â•‘  30% - æ•´åˆæ¸¬è©¦
       â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
     â•‘    Unit Tests       â•‘  60% - å–®å…ƒæ¸¬è©¦
     â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### æ¸¬è©¦è¦†è“‹ç‡ç›®æ¨™
- **ç¸½é«”è¦†è“‹ç‡**: â‰¥ 85%
- **æ ¸å¿ƒåŠŸèƒ½è¦†è“‹ç‡**: â‰¥ 95%
- **é—œéµè·¯å¾‘è¦†è“‹ç‡**: 100%
- **éŒ¯èª¤è™•ç†è¦†è“‹ç‡**: â‰¥ 90%

## ğŸ—ï¸ æ¸¬è©¦æ¡†æ¶æ¶æ§‹

### 1. æŠ€è¡“å †ç–Š
```yaml
Pythonæ¸¬è©¦:
  - pytest: ä¸»è¦æ¸¬è©¦æ¡†æ¶
  - pytest-cov: è¦†è“‹ç‡å ±å‘Š
  - pytest-xdist: ä¸¦è¡Œæ¸¬è©¦
  - pytest-benchmark: æ•ˆèƒ½æ¸¬è©¦
  - pytest-mock: Mockæ¡†æ¶
  - pytest-asyncio: ç•°æ­¥æ¸¬è©¦

JavaScriptæ¸¬è©¦:
  - Jest: å–®å…ƒæ¸¬è©¦
  - Playwright: E2Eæ¸¬è©¦
  - React Testing Library: çµ„ä»¶æ¸¬è©¦
  - Electron Playwright: Electronæ‡‰ç”¨æ¸¬è©¦

æ•ˆèƒ½æ¸¬è©¦:
  - Locust: è² è¼‰æ¸¬è©¦
  - memory_profiler: è¨˜æ†¶é«”åˆ†æ
  - line_profiler: ç¨‹å¼ç¢¼æ•ˆèƒ½åˆ†æ
```

### 2. æ¸¬è©¦åˆ†å±¤æ¶æ§‹

```
tests/
â”œâ”€â”€ unit/                      # å–®å…ƒæ¸¬è©¦
â”‚   â”œâ”€â”€ test_audio_processor.py
â”‚   â”œâ”€â”€ test_voice_detector.py
â”‚   â”œâ”€â”€ test_subtitle_formatter.py
â”‚   â””â”€â”€ test_model_manager.py
â”‚
â”œâ”€â”€ integration/               # æ•´åˆæ¸¬è©¦
â”‚   â”œâ”€â”€ test_ipc_communication.py
â”‚   â”œâ”€â”€ test_backend_integration.py
â”‚   â”œâ”€â”€ test_model_loading.py
â”‚   â””â”€â”€ test_file_processing.py
â”‚
â”œâ”€â”€ e2e/                      # ç«¯åˆ°ç«¯æ¸¬è©¦
â”‚   â”œâ”€â”€ test_complete_workflow.py
â”‚   â”œâ”€â”€ test_ui_interactions.py
â”‚   â”œâ”€â”€ test_batch_processing.py
â”‚   â””â”€â”€ test_error_recovery.py
â”‚
â”œâ”€â”€ performance/              # æ•ˆèƒ½æ¸¬è©¦
â”‚   â”œâ”€â”€ test_processing_speed.py
â”‚   â”œâ”€â”€ test_memory_usage.py
â”‚   â”œâ”€â”€ test_concurrent_processing.py
â”‚   â””â”€â”€ test_large_file_handling.py
â”‚
â”œâ”€â”€ security/                 # å®‰å…¨æ¸¬è©¦
â”‚   â”œâ”€â”€ test_input_validation.py
â”‚   â”œâ”€â”€ test_file_upload_security.py
â”‚   â””â”€â”€ test_path_traversal.py
â”‚
â””â”€â”€ fixtures/                 # æ¸¬è©¦è³‡æ–™
    â”œâ”€â”€ audio_samples/
    â”œâ”€â”€ video_samples/
    â”œâ”€â”€ mock_data/
    â””â”€â”€ expected_outputs/
```

## ğŸ§ª è©³ç´°æ¸¬è©¦è¦ç¯„

### 1. å–®å…ƒæ¸¬è©¦ (Unit Tests)

#### éŸ³é »è™•ç†æ¸¬è©¦
```python
# tests/unit/test_audio_processor.py
import pytest
from audio_processor import AudioProcessor

class TestAudioProcessor:
    @pytest.fixture
    def processor(self):
        return AudioProcessor()
    
    def test_dynamic_range_compression(self, processor):
        """æ¸¬è©¦å‹•æ…‹ç¯„åœå£“ç¸®åŠŸèƒ½"""
        audio_data = load_test_audio("sample.wav")
        compressed = processor.compress_dynamic_range(audio_data)
        assert compressed.max() < audio_data.max()
        assert compressed.std() < audio_data.std()
    
    def test_speech_frequency_enhancement(self, processor):
        """æ¸¬è©¦èªéŸ³é »ç‡å¢å¼·"""
        audio_data = load_test_audio("speech.wav")
        enhanced = processor.enhance_speech_frequencies(audio_data)
        speech_freq_energy = calculate_speech_band_energy(enhanced)
        assert speech_freq_energy > calculate_speech_band_energy(audio_data)
    
    @pytest.mark.parametrize("sample_rate", [16000, 22050, 44100, 48000])
    def test_various_sample_rates(self, processor, sample_rate):
        """æ¸¬è©¦ä¸åŒæ¡æ¨£ç‡çš„ç›¸å®¹æ€§"""
        audio_data = generate_test_audio(sample_rate)
        result = processor.process(audio_data, sample_rate)
        assert result is not None
        assert len(result) > 0
```

#### èªéŸ³æª¢æ¸¬æ¸¬è©¦
```python
# tests/unit/test_voice_detector.py
import pytest
import numpy as np
from adaptive_voice_detector import AdaptiveVoiceDetector

class TestAdaptiveVoiceDetector:
    @pytest.fixture
    def detector(self):
        return AdaptiveVoiceDetector()
    
    def test_feature_extraction_dimensions(self, detector):
        """ç¢ºèªç‰¹å¾µç¶­åº¦ç‚º25ç¶­"""
        audio = np.random.randn(16000)  # 1ç§’éŸ³é »
        features = detector.extract_features(audio)
        assert features.shape[-1] == 25
    
    def test_voice_segment_detection_precision(self, detector):
        """æ¸¬è©¦èªéŸ³æ®µæª¢æ¸¬ç²¾åº¦ Â±0.05s"""
        audio_path = "fixtures/audio_samples/speech_with_silence.wav"
        segments = detector.detect_voice_segments(audio_path)
        
        # é©—è­‰æ¯å€‹æ®µè½çš„æ™‚é–“ç²¾åº¦
        for segment in segments:
            assert segment['confidence'] > 0.7
            assert segment['end'] > segment['start']
            # é©—è­‰æ™‚é–“æˆ³ç²¾åº¦åˆ°å°æ•¸é»å¾Œå…©ä½
            assert round(segment['start'], 2) == segment['start']
    
    @pytest.mark.benchmark
    def test_detection_performance(self, detector, benchmark):
        """æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
        audio = np.random.randn(16000 * 60)  # 60ç§’éŸ³é »
        result = benchmark(detector.detect_voice_segments_from_array, audio)
        assert result is not None
```

### 2. æ•´åˆæ¸¬è©¦ (Integration Tests)

#### IPC é€šè¨Šæ¸¬è©¦
```python
# tests/integration/test_ipc_communication.py
import pytest
import json
import asyncio
from electron_backend import ElectronBackend

class TestIPCCommunication:
    @pytest.fixture
    async def backend(self):
        backend = ElectronBackend()
        await backend.initialize()
        yield backend
        await backend.cleanup()
    
    @pytest.mark.asyncio
    async def test_message_format_validation(self, backend):
        """æ¸¬è©¦è¨Šæ¯æ ¼å¼é©—è­‰"""
        valid_message = {
            "files": ["test.mp4"],
            "settings": {"model": "large", "language": "auto"}
        }
        response = await backend.process_message(json.dumps(valid_message))
        assert response["type"] in ["progress", "result", "error"]
    
    @pytest.mark.asyncio
    async def test_concurrent_processing(self, backend):
        """æ¸¬è©¦ä¸¦ç™¼è™•ç†èƒ½åŠ›"""
        tasks = []
        for i in range(5):
            message = {
                "files": [f"test_{i}.mp4"],
                "settings": {"model": "medium"}
            }
            tasks.append(backend.process_message(json.dumps(message)))
        
        results = await asyncio.gather(*tasks)
        assert len(results) == 5
        assert all(r["status"] == "success" for r in results)
    
    @pytest.mark.asyncio
    async def test_error_handling(self, backend):
        """æ¸¬è©¦éŒ¯èª¤è™•ç†æ©Ÿåˆ¶"""
        invalid_message = {"files": None, "settings": {}}
        response = await backend.process_message(json.dumps(invalid_message))
        assert response["type"] == "error"
        assert "message" in response["data"]
```

### 3. ç«¯åˆ°ç«¯æ¸¬è©¦ (E2E Tests)

```python
# tests/e2e/test_complete_workflow.py
import pytest
from playwright.sync_api import Page, expect
import time

class TestCompleteWorkflow:
    @pytest.fixture
    def app_page(self, electron_app):
        """å•Ÿå‹• Electron æ‡‰ç”¨"""
        page = electron_app.firstWindow()
        yield page
    
    def test_single_file_processing(self, app_page: Page):
        """æ¸¬è©¦å–®æª”æ¡ˆå®Œæ•´è™•ç†æµç¨‹"""
        # 1. é–‹å•Ÿæ‡‰ç”¨
        expect(app_page).to_have_title("SRT GO - AI Subtitle Generator")
        
        # 2. ä¸Šå‚³æª”æ¡ˆ
        app_page.set_input_files('input[type="file"]', 'fixtures/video_samples/test.mp4')
        
        # 3. è¨­å®šåƒæ•¸
        app_page.select_option('#model-select', 'large')
        app_page.select_option('#language-select', 'auto')
        app_page.check('#enable-voice-detection')
        
        # 4. é–‹å§‹è™•ç†
        app_page.click('#process-button')
        
        # 5. ç­‰å¾…è™•ç†å®Œæˆï¼ˆæœ€å¤š5åˆ†é˜ï¼‰
        expect(app_page.locator('#progress-bar')).to_have_attribute(
            'value', '100', timeout=300000
        )
        
        # 6. é©—è­‰è¼¸å‡º
        output_text = app_page.locator('#output-preview').text_content()
        assert len(output_text) > 0
        assert '00:00:00,000' in output_text  # SRTæ ¼å¼é©—è­‰
    
    def test_batch_processing(self, app_page: Page):
        """æ¸¬è©¦æ‰¹æ¬¡è™•ç†åŠŸèƒ½"""
        files = [
            'fixtures/video_samples/test1.mp4',
            'fixtures/video_samples/test2.mp4',
            'fixtures/video_samples/test3.mp4'
        ]
        
        # ä¸Šå‚³å¤šå€‹æª”æ¡ˆ
        app_page.set_input_files('input[type="file"]', files)
        app_page.click('#batch-process-button')
        
        # ç›£æ§æ‰¹æ¬¡é€²åº¦
        for i in range(3):
            progress_locator = app_page.locator(f'#file-{i}-progress')
            expect(progress_locator).to_have_attribute('value', '100', timeout=180000)
        
        # é©—è­‰æ‰€æœ‰è¼¸å‡º
        results = app_page.locator('.result-item').all()
        assert len(results) == 3
```

### 4. æ•ˆèƒ½æ¸¬è©¦ (Performance Tests)

```python
# tests/performance/test_processing_speed.py
import pytest
import time
import psutil
import memory_profiler
from simplified_subtitle_core import SimplifiedSubtitleCore

class TestPerformance:
    @pytest.fixture
    def core(self):
        return SimplifiedSubtitleCore()
    
    def test_rtf_requirement(self, core):
        """æ¸¬è©¦ RTF (Real-Time Factor) è¦æ±‚"""
        test_files = [
            ("fixtures/audio_samples/1min.wav", 0.8),   # CPU mode
            ("fixtures/audio_samples/10min.wav", 0.8),
            ("fixtures/audio_samples/30min.wav", 0.8),
        ]
        
        for file_path, max_rtf in test_files:
            start_time = time.time()
            core.process_audio(file_path)
            processing_time = time.time() - start_time
            
            audio_duration = get_audio_duration(file_path)
            rtf = processing_time / audio_duration
            
            assert rtf < max_rtf, f"RTF {rtf:.2f} exceeds limit {max_rtf}"
    
    @memory_profiler.profile
    def test_memory_usage(self, core):
        """æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        # è™•ç†å¤§æª”æ¡ˆ
        large_file = "fixtures/video_samples/2hour_video.mp4"
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        core.process_audio(large_file)
        
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = peak_memory - initial_memory
        
        assert memory_increase < 4096, f"Memory usage {memory_increase}MB exceeds 4GB limit"
    
    def test_concurrent_processing_limit(self, core):
        """æ¸¬è©¦ä¸¦ç™¼è™•ç†é™åˆ¶"""
        from concurrent.futures import ThreadPoolExecutor
        
        files = ["fixtures/audio_samples/test.wav"] * 10
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            start_time = time.time()
            futures = [executor.submit(core.process_audio, f) for f in files]
            results = [f.result() for f in futures]
            total_time = time.time() - start_time
        
        assert all(r is not None for r in results)
        assert total_time < 60, "Concurrent processing too slow"
```

### 5. å®‰å…¨æ¸¬è©¦ (Security Tests)

```python
# tests/security/test_input_validation.py
import pytest
from pathlib import Path

class TestSecurity:
    def test_path_traversal_prevention(self, backend):
        """æ¸¬è©¦è·¯å¾‘éæ­·æ”»æ“Šé˜²è­·"""
        malicious_paths = [
            "../../../etc/passwd",
            "..\\..\\..\\windows\\system32\\config\\sam",
            "file:///etc/passwd",
            "\\\\server\\share\\file"
        ]
        
        for path in malicious_paths:
            with pytest.raises(SecurityError):
                backend.process_file(path)
    
    def test_file_size_limit(self, backend):
        """æ¸¬è©¦æª”æ¡ˆå¤§å°é™åˆ¶"""
        # å‰µå»ºè¶…å¤§æª”æ¡ˆ
        large_file = create_large_file(size_gb=11)  # 11GB
        
        with pytest.raises(FileSizeError):
            backend.process_file(large_file)
    
    def test_malformed_input_handling(self, backend):
        """æ¸¬è©¦ç•°å¸¸è¼¸å…¥è™•ç†"""
        malformed_inputs = [
            None,
            "",
            "not_a_file",
            {"evil": "payload"},
            "<script>alert('xss')</script>"
        ]
        
        for input_data in malformed_inputs:
            result = backend.process_file(input_data)
            assert result["type"] == "error"
            assert "Invalid input" in result["message"]
```

## ğŸ“Š æ¸¬è©¦æŒ‡æ¨™èˆ‡å ±å‘Š

### 1. è¦†è“‹ç‡å ±å‘Šé…ç½®

```python
# pytest.ini
[tool:pytest]
addopts = 
    --cov=srt_whisper_lite
    --cov-report=html
    --cov-report=term-missing
    --cov-report=xml
    --cov-fail-under=85
    --benchmark-only
    --benchmark-autosave

testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
```

### 2. æ¸¬è©¦å ±å‘Šç”Ÿæˆ

```python
# tests/generate_report.py
import json
import datetime
from pathlib import Path

def generate_test_report():
    """ç”Ÿæˆå•†ç”¨ç´šæ¸¬è©¦å ±å‘Š"""
    report = {
        "timestamp": datetime.datetime.now().isoformat(),
        "version": "2.2.1",
        "summary": {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "coverage": 0
        },
        "categories": {
            "unit_tests": {},
            "integration_tests": {},
            "e2e_tests": {},
            "performance_tests": {},
            "security_tests": {}
        },
        "critical_metrics": {
            "rtf_cpu": 0,
            "rtf_gpu": 0,
            "memory_peak": 0,
            "accuracy_rate": 0,
            "error_rate": 0
        }
    }
    
    # æ”¶é›†æ‰€æœ‰æ¸¬è©¦çµæœ
    collect_pytest_results(report)
    collect_coverage_data(report)
    collect_performance_metrics(report)
    
    # ç”ŸæˆHTMLå ±å‘Š
    generate_html_report(report)
    
    # ç”ŸæˆJSONå ±å‘Šä¾›CI/CDä½¿ç”¨
    with open('test_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    return report
```

## ğŸ”„ CI/CD æ•´åˆ

### GitHub Actions é…ç½®

```yaml
# .github/workflows/test.yml
name: Commercial Grade Testing

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: windows-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
        node-version: [16, 18]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
    
    - name: Install dependencies
      run: |
        pip install -r requirements-test.txt
        cd electron-react-app && npm ci
    
    - name: Run unit tests
      run: pytest tests/unit --cov --cov-report=xml
    
    - name: Run integration tests
      run: pytest tests/integration
    
    - name: Run E2E tests
      run: |
        npm run build
        pytest tests/e2e
    
    - name: Run performance tests
      run: pytest tests/performance --benchmark-only
    
    - name: Run security tests
      run: pytest tests/security
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true
    
    - name: Generate test report
      run: python tests/generate_report.py
    
    - name: Upload test report
      uses: actions/upload-artifact@v3
      with:
        name: test-report
        path: |
          test_report.json
          test_report.html
          coverage/
```

## ğŸ¯ æ¸¬è©¦æª¢æŸ¥æ¸…å–®

### ç™¼å¸ƒå‰å¿…é ˆé€šéçš„æ¸¬è©¦

- [ ] **å–®å…ƒæ¸¬è©¦**: 100% é€šéï¼Œè¦†è“‹ç‡ â‰¥ 85%
- [ ] **æ•´åˆæ¸¬è©¦**: 100% é€šé
- [ ] **E2Eæ¸¬è©¦**: æ ¸å¿ƒæµç¨‹ 100% é€šé
- [ ] **æ•ˆèƒ½æ¸¬è©¦**: 
  - [ ] RTF < 0.8 (CPU)
  - [ ] RTF < 0.15 (GPU)
  - [ ] è¨˜æ†¶é«” < 4GB
- [ ] **å®‰å…¨æ¸¬è©¦**: ç„¡é«˜å±æ¼æ´
- [ ] **å›æ­¸æ¸¬è©¦**: ç„¡åŠŸèƒ½é€€åŒ–
- [ ] **å£“åŠ›æ¸¬è©¦**: 10å°æ™‚é€£çºŒé‹è¡Œç„¡å´©æ½°
- [ ] **ç›¸å®¹æ€§æ¸¬è©¦**: Windows 10/11 x64 é€šé

## ğŸ“ˆ æŒçºŒæ”¹é€²

### æ¸¬è©¦å‚µå‹™è¿½è¹¤
```python
# tests/test_debt_tracker.py
def track_test_debt():
    """è¿½è¹¤æ¸¬è©¦å‚µå‹™"""
    debt_items = [
        {"module": "voice_detector", "missing_tests": 5, "priority": "high"},
        {"module": "ipc_handler", "missing_tests": 3, "priority": "medium"},
        {"module": "ui_components", "missing_tests": 8, "priority": "low"}
    ]
    
    total_debt = sum(item["missing_tests"] for item in debt_items)
    print(f"Total test debt: {total_debt} tests")
    
    # ç”Ÿæˆæ”¹é€²è¨ˆåŠƒ
    generate_improvement_plan(debt_items)
```

### æ¸¬è©¦å“è³ªæŒ‡æ¨™
- **æ¸¬è©¦åŸ·è¡Œæ™‚é–“**: < 10åˆ†é˜ï¼ˆå–®å…ƒæ¸¬è©¦ï¼‰
- **æ¸¬è©¦ç©©å®šæ€§**: é€£çºŒ100æ¬¡åŸ·è¡Œç„¡éš¨æ©Ÿå¤±æ•—
- **æ¸¬è©¦å¯ç¶­è­·æ€§**: æ¯å€‹æ¸¬è©¦ < 50è¡Œç¨‹å¼ç¢¼
- **æ¸¬è©¦ç¨ç«‹æ€§**: ç„¡æ¸¬è©¦é–“ä¾è³´

## ğŸš€ åŸ·è¡Œæ¸¬è©¦

```bash
# å®Œæ•´æ¸¬è©¦å¥—ä»¶
pytest tests/ --cov --benchmark-autosave

# å¿«é€Ÿæ¸¬è©¦ï¼ˆé–‹ç™¼æ™‚ï¼‰
pytest tests/unit -x --ff

# ç™¼å¸ƒæ¸¬è©¦
pytest tests/ --strict-markers --tb=short

# ç”Ÿæˆå ±å‘Š
python tests/generate_report.py
```

é€™å€‹å•†ç”¨ç´šæ¸¬è©¦æ¶æ§‹ç¢ºä¿äº† SRT GO çš„å“è³ªã€ç©©å®šæ€§å’Œå¯é æ€§ã€‚